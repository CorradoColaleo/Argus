{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TskIIZXYVMad"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets accelerate peft bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaexHBytVOQR"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"/teamspace/studios/this_studio/dataset_train_2.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veWldCVnZNst"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "#login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIpuNX9LVSa_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Carica il tokenizer pre-addestrato del modello Llama 2 7B in formato Hugging Face.\n",
        "# Il tokenizer serve a convertire testo in token numerici comprensibili dal modello.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# Imposta il token di padding uguale al token di fine sequenza (EOS token).\n",
        "# Questo Ã¨ utile perchÃ© alcuni modelli non hanno un token di padding dedicato.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Funzione per tokenizzare un esempio del dataset\n",
        "def tokenize(example):\n",
        "    # Crea un prompt strutturato a partire dai campi del dataset\n",
        "    # 'instruction', 'input', 'output'. Il prompt ha la forma:\n",
        "    # ### Instruction:\n",
        "    # <istruzione>\n",
        "    # ### Input:\n",
        "    # <input>\n",
        "    # ### Response:\n",
        "    # <output>\n",
        "    # Questo formato Ã¨ spesso usato per addestrare modelli instruction-following.\n",
        "    prompt = f\"### Instruction:\\n{example['instruction']}\\n### Input:\\n{example['text']}\\n### Response:\\n{example['label']}\"\n",
        "    \n",
        "    # Tokenizza il prompt:\n",
        "    # - truncation=True â†’ tronca il testo se supera max_length\n",
        "    # - padding=\"max_length\" â†’ aggiunge padding fino a max_length\n",
        "    # - max_length=1024 â†’ lunghezza massima dei token\n",
        "    # Restituisce un dizionario con input_ids e attention_mask pronto per il modello.\n",
        "    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=1024)\n",
        "\n",
        "\n",
        "# Applica la funzione di tokenizzazione a tutto il dataset.\n",
        "# `dataset.map()` crea un nuovo dataset in cui ogni esempio Ã¨ giÃ  tokenizzato.\n",
        "tokenized_dataset = dataset.map(tokenize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjogu-5vVXdk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Carica il modello Llama-2 7B pre-addestrato in modalitÃ  Causal Language Modeling.\n",
        "# load_in_8bit=True â†’ utilizza la quantizzazione a 8 bit per ridurre l'uso di memoria.\n",
        "# device_map=\"auto\" â†’ assegna automaticamente i layer del modello ai dispositivi disponibili (CPU/GPU).\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Configurazione per il LoRA (Low-Rank Adaptation)\n",
        "# LoRA permette di fare fine-tuning aggiungendo pochi parametri senza aggiornare tutto il modello.\n",
        "peft_config = LoraConfig(\n",
        "    r=8,                        # Rank della matrice di aggiornamento low-rank\n",
        "    lora_alpha=32,               # Moltiplicatore di scaling per stabilizzare lâ€™addestramento LoRA\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Layer del modello dove applicare LoRA (tipicamente Q e V delle attention)\n",
        "    lora_dropout=0.05,           # Dropout applicato ai pesi LoRA per regolarizzazione\n",
        "    bias=\"none\",                 # Non aggiunge bias aggiuntivo nel fine-tuning\n",
        "    task_type=TaskType.CAUSAL_LM # Tipo di task: Causal Language Modeling\n",
        ")\n",
        "\n",
        "# Applica LoRA al modello originale.\n",
        "# Restituisce un modello PEFT che contiene i pesi originali congelati + i parametri LoRA addestrabili.\n",
        "model = get_peft_model(model, peft_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJPPsL3Eoa85"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# Configurazione dei parametri di training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama-finetuned\",  # Cartella dove salvare i checkpoint del modello fine-tuned\n",
        "    per_device_train_batch_size=2,    # Dimensione del batch per GPU/TPU/CPU\n",
        "    gradient_accumulation_steps=4,    # Accumula i gradienti per simulare batch piÃ¹ grandi\n",
        "    num_train_epochs=1,               # Numero di epoche di training sul dataset\n",
        "    learning_rate=2e-4,               # Learning rate per l'ottimizzatore\n",
        "    logging_dir=\"./logs\",             # Cartella per salvare i log di training\n",
        "    logging_steps=10,                 # Frequenza (in step) di scrittura dei log\n",
        "    save_strategy=\"epoch\",            # Salva il modello alla fine di ogni epoca\n",
        "    fp16=True                         # Abilita mixed precision (half precision) per ridurre uso memoria e velocizzare training\n",
        ")\n",
        "\n",
        "# Collator dei dati per il Language Modeling\n",
        "# Prepara batch di input per il modello\n",
        "# mlm=False â†’ modello non usa masked language modeling, adatto a causal LM come Llama\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "# Creazione del Trainer di Hugging Face\n",
        "trainer = Trainer(\n",
        "    model=model,                      # Modello da allenare\n",
        "    args=training_args,                # Parametri di training definiti sopra\n",
        "    train_dataset=tokenized_dataset[\"train\"],  # Dataset di training giÃ  tokenizzato\n",
        "    tokenizer=tokenizer,               # Tokenizer associato al modello\n",
        "    data_collator=data_collator        # Funzione che prepara i batch durante il training\n",
        ")\n",
        "\n",
        "# Avvia il training del modello\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"### Instruction:\n",
        "You are a classification model specializing in emails, and your job is to detect phishing: respond only with \\\"0\\\" if it is not phishing or \\\"1\\\" if it is phishing, without explanations, symbols, additional letters, or other characters.\n",
        "### Input:\n",
        "We attempted to deliver your package today, but were unable to complete the delivery due to missing address information.\n",
        "Please update your delivery details as soon as possible to avoid return of the shipment:\n",
        "Update Delivery Information\n",
        "Thank you for your cooperation,\n",
        "Logistics Service Team\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "enc = tokenizer(                                               # Tokenizza il prompt usando il tokenizer del modello\n",
        "    prompt,                                                    # Testo da tokenizzare\n",
        "    return_tensors=\"pt\",                                       # Richiede tensori PyTorch come output\n",
        "    padding=True,                                              # Applica padding automatico\n",
        ")\n",
        "\n",
        "input_ids = enc.input_ids.cuda()                               # Sposta gli input IDs sulla GPU\n",
        "attention_mask = enc.attention_mask.cuda()                     # Sposta la attention mask sulla GPU\n",
        "\n",
        "outputs = model.generate(                                      # Genera lâ€™output del modello\n",
        "    input_ids=input_ids,                                       # Fornisce i token di input\n",
        "    attention_mask=attention_mask,                             # Fornisce la maschera di attenzione\n",
        "    max_new_tokens=1,                                          # Limita lâ€™output a un solo token (0 o 1)\n",
        "    pad_token_id=tokenizer.eos_token_id                        # Specifica il token di padding\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))  # Decodifica e stampa lâ€™output finale del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJf_usm4VeCO"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"./llama-finetuned\")\n",
        "tokenizer.save_pretrained(\"./llama-finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIGURAZIONE PATH E MODELLO\n",
        "# ----------------------------\n",
        "\n",
        "BASE_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
        "LORA_ADAPTER_PATH = \"./llama-finetuned\"\n",
        "\n",
        "# ----------------------------\n",
        "# CARICAMENTO TOKENIZER\n",
        "# ----------------------------\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LORA_ADAPTER_PATH)\n",
        "\n",
        "# Per LLaMA Ã¨ buona pratica assicurarsi che il pad_token sia definito\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ----------------------------\n",
        "# CARICAMENTO MODELLO BASE\n",
        "# ----------------------------\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    load_in_8bit=True,        # oppure load_in_4bit=True se usi QLoRA\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# CARICAMENTO ADAPTER LoRA\n",
        "# ----------------------------\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    LORA_ADAPTER_PATH\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dataset_metrics(df, name=\"Dataset\"):          # Definisce una funzione che calcola e stampa metriche descrittive di un DataFrame\n",
        "    print(f\"\\nðŸ“Š Metriche - {name}\")               # Stampa il titolo delle metriche, includendo il nome del dataset\n",
        "    print(\"-\" * 50)                               # Stampa una linea separatrice lunga 50 caratteri\n",
        "    print(f\"Numero righe: {df.shape[0]}\")          # Stampa il numero di righe del DataFrame\n",
        "    print(f\"Numero colonne: {df.shape[1]}\")       # Stampa il numero di colonne del DataFrame\n",
        "    print(\"\\nColonne:\")                            # Stampa lâ€™intestazione della sezione colonne\n",
        "    print(df.columns.tolist())                    # Stampa la lista dei nomi delle colonne\n",
        "    print(\"\\nValori mancanti per colonna:\")       # Stampa lâ€™intestazione della sezione sui valori mancanti\n",
        "    print(df.isnull().sum())                      # Calcola e stampa il numero di valori nulli per ciascuna colonna\n",
        "    print(\"\\nTipi di dato:\")                      # Stampa lâ€™intestazione della sezione sui tipi di dato\n",
        "    print(df.dtypes)                              # Stampa il tipo di dato associato a ogni colonna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd                                  # Importa la libreria pandas per la gestione e lâ€™analisi dei dati\n",
        "import csv                                           # Importa il modulo csv (utile per operazioni su file CSV)\n",
        "import sys                                           # Importa il modulo sys per interazioni con il sistema (argomenti, I/O, ecc.)\n",
        "\n",
        "df = pd.read_json(\"dataset_test.json\")               # Carica il file JSON in un DataFrame pandas\n",
        "\n",
        "df = df.rename(columns={\"text\": \"body\"})             # Rinomina la colonna 'text' in 'body' per uniformare lo schema del dataset\n",
        "\n",
        "dataset_metrics(df, \"Dataset Di Test con metadata\")  # Chiama la funzione dataset_metrics per stampare le metriche del dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stampa informazioni iniziali\n",
        "print(\"Numero di righe iniziali:\", len(df))              # Stampa il numero totale di righe del DataFrame originale\n",
        "print(\"Conteggio label iniziali:\")                       # Stampa lâ€™intestazione per il conteggio delle etichette\n",
        "print(df['label'].value_counts())                        # Calcola e stampa la frequenza di ciascun valore della colonna 'label'\n",
        "\n",
        "# Rimuovi righe con label o body vuoti o NaN\n",
        "df_clean = df.dropna(subset=['label', 'body'])           # Rimuove le righe con valori NaN nelle colonne 'label' o 'body'\n",
        "df_clean = df_clean[df_clean['body'].str.strip() != ''] # Filtra le righe in cui 'body' non Ã¨ una stringa vuota o solo spazi\n",
        "\n",
        "# Stampa informazioni dopo pulizia\n",
        "print(\"\\nNumero di righe dopo la pulizia:\", len(df_clean)) # Stampa il numero di righe dopo lâ€™operazione di pulizia\n",
        "print(\"Conteggio label dopo la pulizia:\")                 # Stampa lâ€™intestazione per il conteggio delle etichette post-pulizia\n",
        "print(df_clean['label'].value_counts())                   # Calcola e stampa la frequenza delle etichette nel dataset pulito\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "risposte = []                                   # Lista per memorizzare le predizioni del modello\n",
        "risposte_vere = []                              # Lista per memorizzare le label reali (ground truth)\n",
        "\n",
        "corretti = 0                                   # Contatore delle predizioni corrette\n",
        "righe_valide = 0                               # Contatore delle righe effettivamente valutate\n",
        "\n",
        "dim = len(df)                                  # Numero totale di righe del DataFrame\n",
        "\n",
        "for i in range(0, dim):                        # Cicla su tutte le righe del DataFrame\n",
        "    first_row = df.iloc[i]                     # Estrae la riga i-esima come Series\n",
        "\n",
        "    # Estrai body e label separatamente\n",
        "    body_text = first_row['body']              # Testo dellâ€™email da classificare\n",
        "    label = first_row['label']                 # Label attesa (0 o 1)\n",
        "\n",
        "    #print(\"Body:\\n\", body_text, \"TIPO: \", type(body_text))\n",
        "    #print(\"\\nLabel Attesa:\\n\", label_text, \"TIPO: \", type(label_text))\n",
        "    # Commenti di debug (attualmente disabilitati)\n",
        "\n",
        "    prompt = f\"\"\"                              # Costruisce dinamicamente il prompt per il modello\n",
        "### Instruction:\n",
        "You are a classification model specializing in emails, and your job is to detect phishing: respond only with \\\"0\\\" if it is not phishing or \\\"1\\\" if it is phishing, without explanations, symbols, additional letters, or other characters.\n",
        "### Input:\n",
        "{body_text}                                    # Inserisce il testo dellâ€™email nel prompt\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    #print(prompt)\n",
        "    # Debug opzionale per visualizzare il prompt completo\n",
        "\n",
        "    enc = tokenizer(                           # Tokenizza il prompt\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",                   # Restituisce tensori PyTorch\n",
        "        padding=True,                          # Applica padding automatico\n",
        "    )\n",
        "\n",
        "    num_tokens = enc.input_ids.shape[1]        # Calcola il numero totale di token del prompt\n",
        "\n",
        "    if num_tokens > 1024:                       # Controlla il limite massimo di token consentiti\n",
        "        # Salta questa riga\n",
        "        continue                               # Esclude lâ€™email troppo lunga dalla valutazione\n",
        "\n",
        "    righe_valide += 1                          # Incrementa il contatore delle righe valide\n",
        "\n",
        "    input_ids = enc.input_ids.cuda()           # Sposta gli input IDs sulla GPU\n",
        "    attention_mask = enc.attention_mask.cuda() # Sposta la attention mask sulla GPU\n",
        "\n",
        "    outputs = model.generate(                  # Genera la risposta del modello\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=1,                      # Limita lâ€™output a un solo token (0 o 1)\n",
        "        pad_token_id=tokenizer.eos_token_id    # Specifica il token di padding\n",
        "    )\n",
        "    \n",
        "    risposta = tokenizer.decode(               # Decodifica lâ€™output del modello in testo\n",
        "        outputs[0], \n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    risultato = risposta.split(                # Estrae la parte successiva a \"### Response:\"\n",
        "        \"### Response:\", \n",
        "        1\n",
        "    )[1].strip()\n",
        "\n",
        "    try:\n",
        "        risultato = float(risultato)           # Converte la risposta in valore numerico\n",
        "\n",
        "        if abs(risultato - label) <= 0.1:      # Verifica se la predizione coincide con la label\n",
        "            corretti = corretti + 1             # Incrementa il contatore dei corretti\n",
        "    except:\n",
        "        risultato = \"Formato Sbagliato\"        # Gestisce output non numerici o malformati\n",
        "\n",
        "    risposte.append(risultato)                 # Aggiunge la predizione alla lista delle risposte\n",
        "    risposte_vere.append(label)                # Aggiunge la label reale alla lista di riferimento\n",
        "\n",
        "    if righe_valide % 100 == 0:                # Ogni 100 righe valide, stampa feedback parziale\n",
        "        accuracy_temp = (\n",
        "            corretti / righe_valide \n",
        "            if righe_valide > 0 else 0\n",
        "        )                                      # Calcola lâ€™accuracy parziale\n",
        "        formati_sbagliati = risposte.count(\n",
        "            \"Formato Sbagliato\"\n",
        "        )                                      # Conta le risposte con formato errato\n",
        "\n",
        "        print(\n",
        "            f\"[FEEDBACK PARZIALE] \"\n",
        "            f\"Righe valide: {righe_valide} | \"\n",
        "            f\"Corretti: {corretti} | \"\n",
        "            f\"Accuracy: {accuracy_temp:.4f} | \"\n",
        "            f\"Formati sbagliati: {formati_sbagliati}\"\n",
        "        )                                      # Stampa metriche intermedie di valutazione\n",
        "\n",
        "# Costruisci il contenuto da scrivere\n",
        "risultati_del_test = (\n",
        "    \"*\" * 10 + \"\\n\" +\n",
        "    f\"Esito (Accuracy): {corretti / righe_valide if righe_valide > 0 else 0:.4f}\\n\\n\" +\n",
        "    f\"Predizioni Corrette: {corretti}\\n\\n\" +\n",
        "    f\"Predizioni Totali: {righe_valide}\\n\\n\" +\n",
        "    f\"Numero di Formati Sbagliati: {risposte.count('Formato Sbagliato')}\\n\" +\n",
        "    \"*\" * 10\n",
        ")\n",
        "\n",
        "# File di output per salvare i risultati\n",
        "output_file = r\"C:\\\\Users\\\\corra\\\\Desktop\\\\universitÃ \\\\AISE\\\\progetto\\\\Argus\\\\risultati_test.txt\"\n",
        "\n",
        "# Scrivi su file\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(risultati_del_test)\n",
        "\n",
        "print(f\"Riepilogo scritto su file: {output_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def confronta_liste(lista1, lista2):            # Definisce una funzione per confrontare due liste elemento per elemento\n",
        "    indici_diversi = []                         # Lista che conterrÃ  gli indici in cui le due liste differiscono\n",
        "\n",
        "    max_len = max(len(lista1), len(lista2))     # Calcola la lunghezza massima tra le due liste\n",
        "    for i in range(max_len):                    # Itera su tutti gli indici fino alla lunghezza massima\n",
        "        if i >= len(lista1):\n",
        "            indici_diversi.append(i)            # Aggiunge lâ€™indice se lâ€™elemento manca in lista1\n",
        "        elif i >= len(lista2):\n",
        "            indici_diversi.append(i)            # Aggiunge lâ€™indice se lâ€™elemento manca in lista2\n",
        "        elif lista1[i] != lista2[i]:\n",
        "            indici_diversi.append(i)            # Aggiunge lâ€™indice se gli elementi sono diversi\n",
        "\n",
        "    return indici_diversi                       # Restituisce la lista degli indici con differenze\n",
        "\n",
        "Indici = confronta_liste(risposte, risposte_vere)  # Confronta predizioni e label reali e salva gli indici discordanti\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(Indici)                                              # Stampa la lista degli indici in cui le predizioni differiscono dalle label reali\n",
        "\n",
        "for indice in Indici:                                      # Itera su ciascun indice errato\n",
        "    print(                                                 # Stampa il dettaglio dellâ€™errore per lâ€™indice corrente\n",
        "        \"Indice:\", indice,\n",
        "        \" Risposte \", risposte[indice],\n",
        "        \" Risposta Corretta: \", risposte_vere[indice]\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "Ti diamo il benvenuto in Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

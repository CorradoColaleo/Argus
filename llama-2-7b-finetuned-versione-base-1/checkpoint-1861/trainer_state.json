{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1861,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005375621556242441,
      "grad_norm": 0.9130061268806458,
      "learning_rate": 0.00019903277807630307,
      "loss": 2.4812,
      "step": 10
    },
    {
      "epoch": 0.010751243112484881,
      "grad_norm": 0.5986878275871277,
      "learning_rate": 0.00019795808704997314,
      "loss": 1.994,
      "step": 20
    },
    {
      "epoch": 0.01612686466872732,
      "grad_norm": 0.5734834671020508,
      "learning_rate": 0.0001968833960236432,
      "loss": 1.8563,
      "step": 30
    },
    {
      "epoch": 0.021502486224969763,
      "grad_norm": 0.6314473152160645,
      "learning_rate": 0.00019580870499731327,
      "loss": 1.7632,
      "step": 40
    },
    {
      "epoch": 0.026878107781212204,
      "grad_norm": 0.5519199967384338,
      "learning_rate": 0.00019473401397098334,
      "loss": 1.7905,
      "step": 50
    },
    {
      "epoch": 0.03225372933745464,
      "grad_norm": 0.5793495178222656,
      "learning_rate": 0.00019365932294465344,
      "loss": 1.7402,
      "step": 60
    },
    {
      "epoch": 0.03762935089369708,
      "grad_norm": 0.7583325505256653,
      "learning_rate": 0.0001925846319183235,
      "loss": 1.7873,
      "step": 70
    },
    {
      "epoch": 0.043004972449939526,
      "grad_norm": 0.6304438710212708,
      "learning_rate": 0.00019150994089199357,
      "loss": 1.8628,
      "step": 80
    },
    {
      "epoch": 0.048380594006181964,
      "grad_norm": 0.6045934557914734,
      "learning_rate": 0.00019043524986566364,
      "loss": 1.8608,
      "step": 90
    },
    {
      "epoch": 0.05375621556242441,
      "grad_norm": 0.7902737259864807,
      "learning_rate": 0.0001893605588393337,
      "loss": 1.8318,
      "step": 100
    },
    {
      "epoch": 0.05913183711866685,
      "grad_norm": 0.8891991972923279,
      "learning_rate": 0.00018828586781300378,
      "loss": 1.8041,
      "step": 110
    },
    {
      "epoch": 0.06450745867490928,
      "grad_norm": 0.6544773578643799,
      "learning_rate": 0.00018721117678667385,
      "loss": 1.774,
      "step": 120
    },
    {
      "epoch": 0.06988308023115172,
      "grad_norm": 0.6956519484519958,
      "learning_rate": 0.00018613648576034392,
      "loss": 1.7031,
      "step": 130
    },
    {
      "epoch": 0.07525870178739416,
      "grad_norm": 0.6266637444496155,
      "learning_rate": 0.00018506179473401396,
      "loss": 1.8015,
      "step": 140
    },
    {
      "epoch": 0.08063432334363661,
      "grad_norm": 0.5338963270187378,
      "learning_rate": 0.00018398710370768403,
      "loss": 1.7769,
      "step": 150
    },
    {
      "epoch": 0.08600994489987905,
      "grad_norm": 0.718939483165741,
      "learning_rate": 0.00018291241268135412,
      "loss": 1.7833,
      "step": 160
    },
    {
      "epoch": 0.09138556645612149,
      "grad_norm": 0.8547254204750061,
      "learning_rate": 0.0001818377216550242,
      "loss": 1.7569,
      "step": 170
    },
    {
      "epoch": 0.09676118801236393,
      "grad_norm": 0.6148881912231445,
      "learning_rate": 0.00018076303062869426,
      "loss": 1.6903,
      "step": 180
    },
    {
      "epoch": 0.10213680956860637,
      "grad_norm": 1.0370869636535645,
      "learning_rate": 0.00017968833960236433,
      "loss": 1.7195,
      "step": 190
    },
    {
      "epoch": 0.10751243112484882,
      "grad_norm": 0.7251562476158142,
      "learning_rate": 0.0001786136485760344,
      "loss": 1.7503,
      "step": 200
    },
    {
      "epoch": 0.11288805268109126,
      "grad_norm": 0.8604989647865295,
      "learning_rate": 0.00017753895754970447,
      "loss": 1.6501,
      "step": 210
    },
    {
      "epoch": 0.1182636742373337,
      "grad_norm": 0.5141726136207581,
      "learning_rate": 0.00017646426652337453,
      "loss": 1.7017,
      "step": 220
    },
    {
      "epoch": 0.12363929579357613,
      "grad_norm": 0.7896981239318848,
      "learning_rate": 0.0001753895754970446,
      "loss": 1.9029,
      "step": 230
    },
    {
      "epoch": 0.12901491734981857,
      "grad_norm": 0.7355912923812866,
      "learning_rate": 0.00017431488447071467,
      "loss": 1.861,
      "step": 240
    },
    {
      "epoch": 0.134390538906061,
      "grad_norm": 0.6615922451019287,
      "learning_rate": 0.00017324019344438477,
      "loss": 1.8866,
      "step": 250
    },
    {
      "epoch": 0.13976616046230345,
      "grad_norm": 0.6224141716957092,
      "learning_rate": 0.00017216550241805483,
      "loss": 1.7304,
      "step": 260
    },
    {
      "epoch": 0.14514178201854588,
      "grad_norm": 0.7402507066726685,
      "learning_rate": 0.0001710908113917249,
      "loss": 1.7144,
      "step": 270
    },
    {
      "epoch": 0.15051740357478832,
      "grad_norm": 0.7283585667610168,
      "learning_rate": 0.00017001612036539497,
      "loss": 1.6649,
      "step": 280
    },
    {
      "epoch": 0.1558930251310308,
      "grad_norm": 0.7173032164573669,
      "learning_rate": 0.00016894142933906504,
      "loss": 1.7664,
      "step": 290
    },
    {
      "epoch": 0.16126864668727323,
      "grad_norm": 0.5543572306632996,
      "learning_rate": 0.00016786673831273508,
      "loss": 1.6581,
      "step": 300
    },
    {
      "epoch": 0.16664426824351566,
      "grad_norm": 0.7292329668998718,
      "learning_rate": 0.00016679204728640515,
      "loss": 1.7497,
      "step": 310
    },
    {
      "epoch": 0.1720198897997581,
      "grad_norm": 0.9028235077857971,
      "learning_rate": 0.00016571735626007522,
      "loss": 1.613,
      "step": 320
    },
    {
      "epoch": 0.17739551135600054,
      "grad_norm": 0.5992484092712402,
      "learning_rate": 0.0001646426652337453,
      "loss": 1.5675,
      "step": 330
    },
    {
      "epoch": 0.18277113291224298,
      "grad_norm": 0.6093884706497192,
      "learning_rate": 0.00016356797420741538,
      "loss": 1.7265,
      "step": 340
    },
    {
      "epoch": 0.18814675446848542,
      "grad_norm": 0.671703040599823,
      "learning_rate": 0.00016249328318108545,
      "loss": 1.5919,
      "step": 350
    },
    {
      "epoch": 0.19352237602472785,
      "grad_norm": 0.7054685354232788,
      "learning_rate": 0.00016141859215475552,
      "loss": 1.6889,
      "step": 360
    },
    {
      "epoch": 0.1988979975809703,
      "grad_norm": 0.8364081978797913,
      "learning_rate": 0.0001603439011284256,
      "loss": 1.72,
      "step": 370
    },
    {
      "epoch": 0.20427361913721273,
      "grad_norm": 0.8534377813339233,
      "learning_rate": 0.00015926921010209566,
      "loss": 1.6871,
      "step": 380
    },
    {
      "epoch": 0.20964924069345517,
      "grad_norm": 0.6902211308479309,
      "learning_rate": 0.00015819451907576572,
      "loss": 1.6806,
      "step": 390
    },
    {
      "epoch": 0.21502486224969763,
      "grad_norm": 0.6604453921318054,
      "learning_rate": 0.0001571198280494358,
      "loss": 1.6889,
      "step": 400
    },
    {
      "epoch": 0.22040048380594007,
      "grad_norm": 0.601283073425293,
      "learning_rate": 0.00015604513702310586,
      "loss": 1.6809,
      "step": 410
    },
    {
      "epoch": 0.2257761053621825,
      "grad_norm": 0.6194453835487366,
      "learning_rate": 0.00015497044599677593,
      "loss": 1.5898,
      "step": 420
    },
    {
      "epoch": 0.23115172691842495,
      "grad_norm": 0.7412668466567993,
      "learning_rate": 0.000153895754970446,
      "loss": 1.6725,
      "step": 430
    },
    {
      "epoch": 0.2365273484746674,
      "grad_norm": 0.6931532025337219,
      "learning_rate": 0.0001528210639441161,
      "loss": 1.6213,
      "step": 440
    },
    {
      "epoch": 0.24190297003090983,
      "grad_norm": 0.6565243601799011,
      "learning_rate": 0.00015174637291778616,
      "loss": 1.5398,
      "step": 450
    },
    {
      "epoch": 0.24727859158715226,
      "grad_norm": 0.7358969449996948,
      "learning_rate": 0.00015067168189145623,
      "loss": 1.691,
      "step": 460
    },
    {
      "epoch": 0.2526542131433947,
      "grad_norm": 0.6543692946434021,
      "learning_rate": 0.00014959699086512627,
      "loss": 1.6051,
      "step": 470
    },
    {
      "epoch": 0.25802983469963714,
      "grad_norm": 0.7836200594902039,
      "learning_rate": 0.00014852229983879634,
      "loss": 1.8091,
      "step": 480
    },
    {
      "epoch": 0.2634054562558796,
      "grad_norm": 0.6961868405342102,
      "learning_rate": 0.0001474476088124664,
      "loss": 1.6829,
      "step": 490
    },
    {
      "epoch": 0.268781077812122,
      "grad_norm": 0.7990560531616211,
      "learning_rate": 0.00014637291778613648,
      "loss": 1.5945,
      "step": 500
    },
    {
      "epoch": 0.27415669936836445,
      "grad_norm": 0.806333065032959,
      "learning_rate": 0.00014529822675980655,
      "loss": 1.6778,
      "step": 510
    },
    {
      "epoch": 0.2795323209246069,
      "grad_norm": 0.9007812738418579,
      "learning_rate": 0.00014422353573347662,
      "loss": 1.598,
      "step": 520
    },
    {
      "epoch": 0.28490794248084933,
      "grad_norm": 0.7913812398910522,
      "learning_rate": 0.0001431488447071467,
      "loss": 1.654,
      "step": 530
    },
    {
      "epoch": 0.29028356403709177,
      "grad_norm": 0.7168578505516052,
      "learning_rate": 0.00014207415368081678,
      "loss": 1.6553,
      "step": 540
    },
    {
      "epoch": 0.2956591855933342,
      "grad_norm": 0.6954566240310669,
      "learning_rate": 0.00014099946265448685,
      "loss": 1.7509,
      "step": 550
    },
    {
      "epoch": 0.30103480714957664,
      "grad_norm": 0.7561463713645935,
      "learning_rate": 0.00013992477162815692,
      "loss": 1.6051,
      "step": 560
    },
    {
      "epoch": 0.30641042870581914,
      "grad_norm": 0.6593548655509949,
      "learning_rate": 0.00013885008060182698,
      "loss": 1.7344,
      "step": 570
    },
    {
      "epoch": 0.3117860502620616,
      "grad_norm": 0.6605856418609619,
      "learning_rate": 0.00013777538957549705,
      "loss": 1.6302,
      "step": 580
    },
    {
      "epoch": 0.317161671818304,
      "grad_norm": 0.7393540143966675,
      "learning_rate": 0.00013670069854916712,
      "loss": 1.6723,
      "step": 590
    },
    {
      "epoch": 0.32253729337454645,
      "grad_norm": 0.6484012007713318,
      "learning_rate": 0.0001356260075228372,
      "loss": 1.656,
      "step": 600
    },
    {
      "epoch": 0.3279129149307889,
      "grad_norm": 0.6454870700836182,
      "learning_rate": 0.00013455131649650726,
      "loss": 1.5511,
      "step": 610
    },
    {
      "epoch": 0.33328853648703133,
      "grad_norm": 0.5903611183166504,
      "learning_rate": 0.00013347662547017735,
      "loss": 1.527,
      "step": 620
    },
    {
      "epoch": 0.33866415804327377,
      "grad_norm": 0.8754059076309204,
      "learning_rate": 0.0001324019344438474,
      "loss": 1.5754,
      "step": 630
    },
    {
      "epoch": 0.3440397795995162,
      "grad_norm": 0.7342609763145447,
      "learning_rate": 0.00013132724341751746,
      "loss": 1.796,
      "step": 640
    },
    {
      "epoch": 0.34941540115575864,
      "grad_norm": 0.776323676109314,
      "learning_rate": 0.00013025255239118753,
      "loss": 1.6403,
      "step": 650
    },
    {
      "epoch": 0.3547910227120011,
      "grad_norm": 0.9227915406227112,
      "learning_rate": 0.0001291778613648576,
      "loss": 1.5671,
      "step": 660
    },
    {
      "epoch": 0.3601666442682435,
      "grad_norm": 0.806039571762085,
      "learning_rate": 0.00012810317033852767,
      "loss": 1.6212,
      "step": 670
    },
    {
      "epoch": 0.36554226582448596,
      "grad_norm": 0.915001392364502,
      "learning_rate": 0.00012702847931219774,
      "loss": 1.6359,
      "step": 680
    },
    {
      "epoch": 0.3709178873807284,
      "grad_norm": 1.0315157175064087,
      "learning_rate": 0.0001259537882858678,
      "loss": 1.6035,
      "step": 690
    },
    {
      "epoch": 0.37629350893697083,
      "grad_norm": 0.6574068069458008,
      "learning_rate": 0.00012487909725953787,
      "loss": 1.7498,
      "step": 700
    },
    {
      "epoch": 0.38166913049321327,
      "grad_norm": 0.7254543304443359,
      "learning_rate": 0.00012380440623320794,
      "loss": 1.6329,
      "step": 710
    },
    {
      "epoch": 0.3870447520494557,
      "grad_norm": 0.8647783994674683,
      "learning_rate": 0.00012272971520687804,
      "loss": 1.7578,
      "step": 720
    },
    {
      "epoch": 0.39242037360569815,
      "grad_norm": 0.753470242023468,
      "learning_rate": 0.00012165502418054811,
      "loss": 1.7008,
      "step": 730
    },
    {
      "epoch": 0.3977959951619406,
      "grad_norm": 0.8385739922523499,
      "learning_rate": 0.00012058033315421818,
      "loss": 1.4684,
      "step": 740
    },
    {
      "epoch": 0.403171616718183,
      "grad_norm": 0.7060860395431519,
      "learning_rate": 0.00011950564212788824,
      "loss": 1.6547,
      "step": 750
    },
    {
      "epoch": 0.40854723827442546,
      "grad_norm": 0.7400408387184143,
      "learning_rate": 0.00011843095110155831,
      "loss": 1.5308,
      "step": 760
    },
    {
      "epoch": 0.4139228598306679,
      "grad_norm": 0.7483353614807129,
      "learning_rate": 0.00011735626007522838,
      "loss": 1.5648,
      "step": 770
    },
    {
      "epoch": 0.41929848138691034,
      "grad_norm": 0.6234086751937866,
      "learning_rate": 0.00011628156904889844,
      "loss": 1.514,
      "step": 780
    },
    {
      "epoch": 0.4246741029431528,
      "grad_norm": 0.7210249304771423,
      "learning_rate": 0.0001152068780225685,
      "loss": 1.8129,
      "step": 790
    },
    {
      "epoch": 0.43004972449939527,
      "grad_norm": 0.782059907913208,
      "learning_rate": 0.00011413218699623857,
      "loss": 1.4469,
      "step": 800
    },
    {
      "epoch": 0.4354253460556377,
      "grad_norm": 0.6572837233543396,
      "learning_rate": 0.00011305749596990867,
      "loss": 1.6715,
      "step": 810
    },
    {
      "epoch": 0.44080096761188015,
      "grad_norm": 0.6362775564193726,
      "learning_rate": 0.00011198280494357874,
      "loss": 1.627,
      "step": 820
    },
    {
      "epoch": 0.4461765891681226,
      "grad_norm": 1.0881956815719604,
      "learning_rate": 0.0001109081139172488,
      "loss": 1.7385,
      "step": 830
    },
    {
      "epoch": 0.451552210724365,
      "grad_norm": 0.858774721622467,
      "learning_rate": 0.00010983342289091887,
      "loss": 1.5469,
      "step": 840
    },
    {
      "epoch": 0.45692783228060746,
      "grad_norm": 0.6160925626754761,
      "learning_rate": 0.00010875873186458894,
      "loss": 1.612,
      "step": 850
    },
    {
      "epoch": 0.4623034538368499,
      "grad_norm": 0.6198986172676086,
      "learning_rate": 0.000107684040838259,
      "loss": 1.679,
      "step": 860
    },
    {
      "epoch": 0.46767907539309234,
      "grad_norm": 0.8072633147239685,
      "learning_rate": 0.00010660934981192907,
      "loss": 1.5085,
      "step": 870
    },
    {
      "epoch": 0.4730546969493348,
      "grad_norm": 0.6881699562072754,
      "learning_rate": 0.00010553465878559913,
      "loss": 1.6686,
      "step": 880
    },
    {
      "epoch": 0.4784303185055772,
      "grad_norm": 0.7968171834945679,
      "learning_rate": 0.0001044599677592692,
      "loss": 1.5628,
      "step": 890
    },
    {
      "epoch": 0.48380594006181965,
      "grad_norm": 0.7218589186668396,
      "learning_rate": 0.0001033852767329393,
      "loss": 1.5204,
      "step": 900
    },
    {
      "epoch": 0.4891815616180621,
      "grad_norm": 0.6833581924438477,
      "learning_rate": 0.00010231058570660937,
      "loss": 1.7019,
      "step": 910
    },
    {
      "epoch": 0.4945571831743045,
      "grad_norm": 0.8965012431144714,
      "learning_rate": 0.00010123589468027944,
      "loss": 1.5365,
      "step": 920
    },
    {
      "epoch": 0.49993280473054696,
      "grad_norm": 0.5535350441932678,
      "learning_rate": 0.0001001612036539495,
      "loss": 1.4516,
      "step": 930
    },
    {
      "epoch": 0.5053084262867894,
      "grad_norm": 0.8676831722259521,
      "learning_rate": 9.908651262761956e-05,
      "loss": 1.5049,
      "step": 940
    },
    {
      "epoch": 0.5106840478430319,
      "grad_norm": 1.0434000492095947,
      "learning_rate": 9.801182160128963e-05,
      "loss": 1.63,
      "step": 950
    },
    {
      "epoch": 0.5160596693992743,
      "grad_norm": 0.5473006367683411,
      "learning_rate": 9.69371305749597e-05,
      "loss": 1.7606,
      "step": 960
    },
    {
      "epoch": 0.5214352909555168,
      "grad_norm": 1.0227173566818237,
      "learning_rate": 9.586243954862978e-05,
      "loss": 1.5392,
      "step": 970
    },
    {
      "epoch": 0.5268109125117592,
      "grad_norm": 0.6345211267471313,
      "learning_rate": 9.478774852229985e-05,
      "loss": 1.5931,
      "step": 980
    },
    {
      "epoch": 0.5321865340680016,
      "grad_norm": 0.6777240037918091,
      "learning_rate": 9.371305749596991e-05,
      "loss": 1.4681,
      "step": 990
    },
    {
      "epoch": 0.537562155624244,
      "grad_norm": 0.8596173524856567,
      "learning_rate": 9.263836646963998e-05,
      "loss": 1.6655,
      "step": 1000
    },
    {
      "epoch": 0.5429377771804865,
      "grad_norm": 0.7799498438835144,
      "learning_rate": 9.156367544331005e-05,
      "loss": 1.5609,
      "step": 1010
    },
    {
      "epoch": 0.5483133987367289,
      "grad_norm": 0.7703959941864014,
      "learning_rate": 9.048898441698012e-05,
      "loss": 1.4969,
      "step": 1020
    },
    {
      "epoch": 0.5536890202929714,
      "grad_norm": 0.7670535445213318,
      "learning_rate": 8.941429339065019e-05,
      "loss": 1.6523,
      "step": 1030
    },
    {
      "epoch": 0.5590646418492138,
      "grad_norm": 0.8335424065589905,
      "learning_rate": 8.833960236432026e-05,
      "loss": 1.3758,
      "step": 1040
    },
    {
      "epoch": 0.5644402634054563,
      "grad_norm": 0.8503674864768982,
      "learning_rate": 8.726491133799033e-05,
      "loss": 1.5356,
      "step": 1050
    },
    {
      "epoch": 0.5698158849616987,
      "grad_norm": 0.7751486897468567,
      "learning_rate": 8.61902203116604e-05,
      "loss": 1.5652,
      "step": 1060
    },
    {
      "epoch": 0.5751915065179412,
      "grad_norm": 0.8289251327514648,
      "learning_rate": 8.511552928533048e-05,
      "loss": 1.8009,
      "step": 1070
    },
    {
      "epoch": 0.5805671280741835,
      "grad_norm": 0.5955038666725159,
      "learning_rate": 8.404083825900054e-05,
      "loss": 1.4476,
      "step": 1080
    },
    {
      "epoch": 0.585942749630426,
      "grad_norm": 0.7329281568527222,
      "learning_rate": 8.296614723267061e-05,
      "loss": 1.582,
      "step": 1090
    },
    {
      "epoch": 0.5913183711866684,
      "grad_norm": 0.8795456886291504,
      "learning_rate": 8.189145620634068e-05,
      "loss": 1.7313,
      "step": 1100
    },
    {
      "epoch": 0.5966939927429109,
      "grad_norm": 0.7474023103713989,
      "learning_rate": 8.081676518001075e-05,
      "loss": 1.5667,
      "step": 1110
    },
    {
      "epoch": 0.6020696142991533,
      "grad_norm": 1.1074639558792114,
      "learning_rate": 7.974207415368082e-05,
      "loss": 1.4799,
      "step": 1120
    },
    {
      "epoch": 0.6074452358553958,
      "grad_norm": 0.8676747679710388,
      "learning_rate": 7.866738312735089e-05,
      "loss": 1.6962,
      "step": 1130
    },
    {
      "epoch": 0.6128208574116383,
      "grad_norm": 0.6848458051681519,
      "learning_rate": 7.759269210102096e-05,
      "loss": 1.6358,
      "step": 1140
    },
    {
      "epoch": 0.6181964789678807,
      "grad_norm": 0.7054328918457031,
      "learning_rate": 7.651800107469102e-05,
      "loss": 1.591,
      "step": 1150
    },
    {
      "epoch": 0.6235721005241232,
      "grad_norm": 0.9386595487594604,
      "learning_rate": 7.54433100483611e-05,
      "loss": 1.6204,
      "step": 1160
    },
    {
      "epoch": 0.6289477220803655,
      "grad_norm": 0.9038611650466919,
      "learning_rate": 7.436861902203117e-05,
      "loss": 1.5485,
      "step": 1170
    },
    {
      "epoch": 0.634323343636608,
      "grad_norm": 0.6036266088485718,
      "learning_rate": 7.329392799570124e-05,
      "loss": 1.402,
      "step": 1180
    },
    {
      "epoch": 0.6396989651928504,
      "grad_norm": 1.0133217573165894,
      "learning_rate": 7.221923696937131e-05,
      "loss": 1.7334,
      "step": 1190
    },
    {
      "epoch": 0.6450745867490929,
      "grad_norm": 0.8494884967803955,
      "learning_rate": 7.114454594304138e-05,
      "loss": 1.5563,
      "step": 1200
    },
    {
      "epoch": 0.6504502083053353,
      "grad_norm": 0.9166604280471802,
      "learning_rate": 7.006985491671145e-05,
      "loss": 1.6986,
      "step": 1210
    },
    {
      "epoch": 0.6558258298615778,
      "grad_norm": 0.7241010665893555,
      "learning_rate": 6.899516389038152e-05,
      "loss": 1.662,
      "step": 1220
    },
    {
      "epoch": 0.6612014514178202,
      "grad_norm": 0.7103740572929382,
      "learning_rate": 6.792047286405159e-05,
      "loss": 1.3956,
      "step": 1230
    },
    {
      "epoch": 0.6665770729740627,
      "grad_norm": 0.5452070832252502,
      "learning_rate": 6.684578183772165e-05,
      "loss": 1.4988,
      "step": 1240
    },
    {
      "epoch": 0.671952694530305,
      "grad_norm": 0.7019305229187012,
      "learning_rate": 6.577109081139174e-05,
      "loss": 1.6468,
      "step": 1250
    },
    {
      "epoch": 0.6773283160865475,
      "grad_norm": 0.7261512279510498,
      "learning_rate": 6.46963997850618e-05,
      "loss": 1.8504,
      "step": 1260
    },
    {
      "epoch": 0.6827039376427899,
      "grad_norm": 0.8395404815673828,
      "learning_rate": 6.362170875873187e-05,
      "loss": 1.6476,
      "step": 1270
    },
    {
      "epoch": 0.6880795591990324,
      "grad_norm": 0.8012770414352417,
      "learning_rate": 6.254701773240194e-05,
      "loss": 1.6576,
      "step": 1280
    },
    {
      "epoch": 0.6934551807552748,
      "grad_norm": 0.6032178401947021,
      "learning_rate": 6.147232670607201e-05,
      "loss": 1.7472,
      "step": 1290
    },
    {
      "epoch": 0.6988308023115173,
      "grad_norm": 0.9024198651313782,
      "learning_rate": 6.039763567974208e-05,
      "loss": 1.5686,
      "step": 1300
    },
    {
      "epoch": 0.7042064238677597,
      "grad_norm": 0.8730781674385071,
      "learning_rate": 5.932294465341215e-05,
      "loss": 1.618,
      "step": 1310
    },
    {
      "epoch": 0.7095820454240022,
      "grad_norm": 0.7792965173721313,
      "learning_rate": 5.8248253627082215e-05,
      "loss": 1.5075,
      "step": 1320
    },
    {
      "epoch": 0.7149576669802445,
      "grad_norm": 0.79367995262146,
      "learning_rate": 5.7173562600752284e-05,
      "loss": 1.5851,
      "step": 1330
    },
    {
      "epoch": 0.720333288536487,
      "grad_norm": 0.9796489477157593,
      "learning_rate": 5.609887157442235e-05,
      "loss": 1.4819,
      "step": 1340
    },
    {
      "epoch": 0.7257089100927294,
      "grad_norm": 0.9869778156280518,
      "learning_rate": 5.502418054809243e-05,
      "loss": 1.5176,
      "step": 1350
    },
    {
      "epoch": 0.7310845316489719,
      "grad_norm": 0.7558121681213379,
      "learning_rate": 5.3949489521762496e-05,
      "loss": 1.586,
      "step": 1360
    },
    {
      "epoch": 0.7364601532052144,
      "grad_norm": 0.6355860233306885,
      "learning_rate": 5.2874798495432564e-05,
      "loss": 1.5267,
      "step": 1370
    },
    {
      "epoch": 0.7418357747614568,
      "grad_norm": 1.1100711822509766,
      "learning_rate": 5.180010746910263e-05,
      "loss": 1.565,
      "step": 1380
    },
    {
      "epoch": 0.7472113963176993,
      "grad_norm": 1.0451806783676147,
      "learning_rate": 5.072541644277271e-05,
      "loss": 1.7657,
      "step": 1390
    },
    {
      "epoch": 0.7525870178739417,
      "grad_norm": 0.65185546875,
      "learning_rate": 4.965072541644277e-05,
      "loss": 1.5175,
      "step": 1400
    },
    {
      "epoch": 0.7579626394301842,
      "grad_norm": 0.6763515472412109,
      "learning_rate": 4.8576034390112845e-05,
      "loss": 1.5563,
      "step": 1410
    },
    {
      "epoch": 0.7633382609864265,
      "grad_norm": 0.9196968078613281,
      "learning_rate": 4.7501343363782914e-05,
      "loss": 1.5886,
      "step": 1420
    },
    {
      "epoch": 0.768713882542669,
      "grad_norm": 0.6821900606155396,
      "learning_rate": 4.642665233745299e-05,
      "loss": 1.5983,
      "step": 1430
    },
    {
      "epoch": 0.7740895040989114,
      "grad_norm": 0.6541149020195007,
      "learning_rate": 4.535196131112305e-05,
      "loss": 1.6685,
      "step": 1440
    },
    {
      "epoch": 0.7794651256551539,
      "grad_norm": 0.7974185347557068,
      "learning_rate": 4.4277270284793126e-05,
      "loss": 1.5302,
      "step": 1450
    },
    {
      "epoch": 0.7848407472113963,
      "grad_norm": 0.8814598917961121,
      "learning_rate": 4.3202579258463194e-05,
      "loss": 1.6113,
      "step": 1460
    },
    {
      "epoch": 0.7902163687676388,
      "grad_norm": 0.8914104700088501,
      "learning_rate": 4.212788823213326e-05,
      "loss": 1.5519,
      "step": 1470
    },
    {
      "epoch": 0.7955919903238812,
      "grad_norm": 0.6642026901245117,
      "learning_rate": 4.105319720580333e-05,
      "loss": 1.5068,
      "step": 1480
    },
    {
      "epoch": 0.8009676118801237,
      "grad_norm": 0.6183596849441528,
      "learning_rate": 3.99785061794734e-05,
      "loss": 1.6011,
      "step": 1490
    },
    {
      "epoch": 0.806343233436366,
      "grad_norm": 0.7995845675468445,
      "learning_rate": 3.8903815153143475e-05,
      "loss": 1.463,
      "step": 1500
    },
    {
      "epoch": 0.8117188549926085,
      "grad_norm": 1.057592511177063,
      "learning_rate": 3.7829124126813543e-05,
      "loss": 1.6345,
      "step": 1510
    },
    {
      "epoch": 0.8170944765488509,
      "grad_norm": 1.0644733905792236,
      "learning_rate": 3.675443310048361e-05,
      "loss": 1.4279,
      "step": 1520
    },
    {
      "epoch": 0.8224700981050934,
      "grad_norm": 0.6288965940475464,
      "learning_rate": 3.567974207415368e-05,
      "loss": 1.5521,
      "step": 1530
    },
    {
      "epoch": 0.8278457196613358,
      "grad_norm": 0.7157094478607178,
      "learning_rate": 3.460505104782375e-05,
      "loss": 1.6176,
      "step": 1540
    },
    {
      "epoch": 0.8332213412175783,
      "grad_norm": 0.9025920629501343,
      "learning_rate": 3.3530360021493824e-05,
      "loss": 1.608,
      "step": 1550
    },
    {
      "epoch": 0.8385969627738207,
      "grad_norm": 1.0174498558044434,
      "learning_rate": 3.245566899516389e-05,
      "loss": 1.4747,
      "step": 1560
    },
    {
      "epoch": 0.8439725843300632,
      "grad_norm": 0.8025014996528625,
      "learning_rate": 3.138097796883396e-05,
      "loss": 1.5742,
      "step": 1570
    },
    {
      "epoch": 0.8493482058863056,
      "grad_norm": 1.1388620138168335,
      "learning_rate": 3.030628694250403e-05,
      "loss": 1.592,
      "step": 1580
    },
    {
      "epoch": 0.854723827442548,
      "grad_norm": 0.7438000440597534,
      "learning_rate": 2.9231595916174105e-05,
      "loss": 1.4964,
      "step": 1590
    },
    {
      "epoch": 0.8600994489987905,
      "grad_norm": 0.679034411907196,
      "learning_rate": 2.815690488984417e-05,
      "loss": 1.6316,
      "step": 1600
    },
    {
      "epoch": 0.8654750705550329,
      "grad_norm": 0.9822450876235962,
      "learning_rate": 2.708221386351424e-05,
      "loss": 1.6282,
      "step": 1610
    },
    {
      "epoch": 0.8708506921112754,
      "grad_norm": 0.7616355419158936,
      "learning_rate": 2.600752283718431e-05,
      "loss": 1.6578,
      "step": 1620
    },
    {
      "epoch": 0.8762263136675178,
      "grad_norm": 0.6515152454376221,
      "learning_rate": 2.4932831810854382e-05,
      "loss": 1.6469,
      "step": 1630
    },
    {
      "epoch": 0.8816019352237603,
      "grad_norm": 1.0142697095870972,
      "learning_rate": 2.385814078452445e-05,
      "loss": 1.5846,
      "step": 1640
    },
    {
      "epoch": 0.8869775567800027,
      "grad_norm": 0.8472923040390015,
      "learning_rate": 2.278344975819452e-05,
      "loss": 1.5147,
      "step": 1650
    },
    {
      "epoch": 0.8923531783362452,
      "grad_norm": 0.7842655181884766,
      "learning_rate": 2.170875873186459e-05,
      "loss": 1.6527,
      "step": 1660
    },
    {
      "epoch": 0.8977287998924876,
      "grad_norm": 0.7692236304283142,
      "learning_rate": 2.063406770553466e-05,
      "loss": 1.5948,
      "step": 1670
    },
    {
      "epoch": 0.90310442144873,
      "grad_norm": 0.689450740814209,
      "learning_rate": 1.955937667920473e-05,
      "loss": 1.5161,
      "step": 1680
    },
    {
      "epoch": 0.9084800430049724,
      "grad_norm": 1.365365743637085,
      "learning_rate": 1.84846856528748e-05,
      "loss": 1.6593,
      "step": 1690
    },
    {
      "epoch": 0.9138556645612149,
      "grad_norm": 0.8219189643859863,
      "learning_rate": 1.740999462654487e-05,
      "loss": 1.568,
      "step": 1700
    },
    {
      "epoch": 0.9192312861174573,
      "grad_norm": 0.8132619261741638,
      "learning_rate": 1.6335303600214937e-05,
      "loss": 1.4808,
      "step": 1710
    },
    {
      "epoch": 0.9246069076736998,
      "grad_norm": 0.6118926405906677,
      "learning_rate": 1.526061257388501e-05,
      "loss": 1.6169,
      "step": 1720
    },
    {
      "epoch": 0.9299825292299422,
      "grad_norm": 0.7659835815429688,
      "learning_rate": 1.4185921547555079e-05,
      "loss": 1.391,
      "step": 1730
    },
    {
      "epoch": 0.9353581507861847,
      "grad_norm": 0.9751192927360535,
      "learning_rate": 1.3111230521225149e-05,
      "loss": 1.562,
      "step": 1740
    },
    {
      "epoch": 0.940733772342427,
      "grad_norm": 1.1379897594451904,
      "learning_rate": 1.203653949489522e-05,
      "loss": 1.5592,
      "step": 1750
    },
    {
      "epoch": 0.9461093938986695,
      "grad_norm": 0.8082565665245056,
      "learning_rate": 1.0961848468565288e-05,
      "loss": 1.3937,
      "step": 1760
    },
    {
      "epoch": 0.9514850154549119,
      "grad_norm": 0.8441205024719238,
      "learning_rate": 9.887157442235358e-06,
      "loss": 1.4771,
      "step": 1770
    },
    {
      "epoch": 0.9568606370111544,
      "grad_norm": 0.9196988344192505,
      "learning_rate": 8.812466415905428e-06,
      "loss": 1.4997,
      "step": 1780
    },
    {
      "epoch": 0.9622362585673968,
      "grad_norm": 0.9989079236984253,
      "learning_rate": 7.737775389575496e-06,
      "loss": 1.4801,
      "step": 1790
    },
    {
      "epoch": 0.9676118801236393,
      "grad_norm": 0.6689873337745667,
      "learning_rate": 6.663084363245567e-06,
      "loss": 1.6705,
      "step": 1800
    },
    {
      "epoch": 0.9729875016798817,
      "grad_norm": 0.6632188558578491,
      "learning_rate": 5.588393336915637e-06,
      "loss": 1.6486,
      "step": 1810
    },
    {
      "epoch": 0.9783631232361242,
      "grad_norm": 0.721428394317627,
      "learning_rate": 4.513702310585707e-06,
      "loss": 1.5491,
      "step": 1820
    },
    {
      "epoch": 0.9837387447923667,
      "grad_norm": 0.7297703623771667,
      "learning_rate": 3.4390112842557767e-06,
      "loss": 1.5351,
      "step": 1830
    },
    {
      "epoch": 0.989114366348609,
      "grad_norm": 1.0537043809890747,
      "learning_rate": 2.3643202579258465e-06,
      "loss": 1.5101,
      "step": 1840
    },
    {
      "epoch": 0.9944899879048515,
      "grad_norm": 0.685673177242279,
      "learning_rate": 1.2896292315959162e-06,
      "loss": 1.5753,
      "step": 1850
    },
    {
      "epoch": 0.9998656094610939,
      "grad_norm": 0.7104600667953491,
      "learning_rate": 2.1493820526598604e-07,
      "loss": 1.5821,
      "step": 1860
    }
  ],
  "logging_steps": 10,
  "max_steps": 1861,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.022630113037517e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}

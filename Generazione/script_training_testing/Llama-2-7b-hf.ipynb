{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TskIIZXYVMad"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets accelerate peft bitsandbytes mauve-text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaexHBytVOQR"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"/teamspace/studios/this_studio/dataset_train.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veWldCVnZNst"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "#login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIpuNX9LVSa_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Carica il tokenizer pre-addestrato del modello Llama 2 7B in formato Hugging Face.\n",
        "# Il tokenizer serve a convertire testo in token numerici comprensibili dal modello.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# Imposta il token di padding uguale al token di fine sequenza (EOS token).\n",
        "# Questo Ã¨ utile perchÃ© alcuni modelli non hanno un token di padding dedicato.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Funzione per tokenizzare un esempio del dataset\n",
        "def tokenize(example):\n",
        "    # Crea un prompt strutturato a partire dai campi del dataset\n",
        "    # 'instruction', 'input', 'output'. Il prompt ha la forma:\n",
        "    # ### Instruction:\n",
        "    # <istruzione>\n",
        "    # ### Input:\n",
        "    # <input>\n",
        "    # ### Response:\n",
        "    # <output>\n",
        "    # Questo formato Ã¨ spesso usato per addestrare modelli instruction-following.\n",
        "    prompt = f\"### Instruction:\\n{example['instruction']}\\n### Input:\\n{example['input']}\\n### Response:\\n{example['output']}\"\n",
        "    \n",
        "    # Tokenizza il prompt:\n",
        "    # - truncation=True â†’ tronca il testo se supera max_length\n",
        "    # - padding=\"max_length\" â†’ aggiunge padding fino a max_length\n",
        "    # - max_length=1024 â†’ lunghezza massima dei token\n",
        "    # Restituisce un dizionario con input_ids e attention_mask pronto per il modello.\n",
        "    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=1024)\n",
        "\n",
        "\n",
        "# Applica la funzione di tokenizzazione a tutto il dataset.\n",
        "# `dataset.map()` crea un nuovo dataset in cui ogni esempio Ã¨ giÃ  tokenizzato.\n",
        "tokenized_dataset = dataset.map(tokenize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjogu-5vVXdk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Carica il modello Llama-2 7B pre-addestrato in modalitÃ  Causal Language Modeling.\n",
        "# load_in_8bit=True â†’ utilizza la quantizzazione a 8 bit per ridurre l'uso di memoria.\n",
        "# device_map=\"auto\" â†’ assegna automaticamente i layer del modello ai dispositivi disponibili (CPU/GPU).\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Configurazione per il LoRA (Low-Rank Adaptation)\n",
        "# LoRA permette di fare fine-tuning aggiungendo pochi parametri senza aggiornare tutto il modello.\n",
        "peft_config = LoraConfig(\n",
        "    r=8,                        # Rank della matrice di aggiornamento low-rank\n",
        "    lora_alpha=32,               # Moltiplicatore di scaling per stabilizzare lâ€™addestramento LoRA\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Layer del modello dove applicare LoRA (tipicamente Q e V delle attention)\n",
        "    lora_dropout=0.05,           # Dropout applicato ai pesi LoRA per regolarizzazione\n",
        "    bias=\"none\",                 # Non aggiunge bias aggiuntivo nel fine-tuning\n",
        "    task_type=TaskType.CAUSAL_LM # Tipo di task: Causal Language Modeling\n",
        ")\n",
        "\n",
        "# Applica LoRA al modello originale.\n",
        "# Restituisce un modello PEFT che contiene i pesi originali congelati + i parametri LoRA addestrabili.\n",
        "model = get_peft_model(model, peft_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJPPsL3Eoa85"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# Configurazione dei parametri di training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama-finetuned-generation\",  # Cartella dove salvare i checkpoint del modello fine-tuned\n",
        "    per_device_train_batch_size=2,    # Dimensione del batch per GPU/TPU/CPU\n",
        "    gradient_accumulation_steps=4,    # Accumula i gradienti per simulare batch piÃ¹ grandi\n",
        "    num_train_epochs=1,               # Numero di epoche di training sul dataset\n",
        "    learning_rate=2e-4,               # Learning rate per l'ottimizzatore\n",
        "    logging_dir=\"./logs-generation\",             # Cartella per salvare i log di training\n",
        "    logging_steps=10,                 # Frequenza (in step) di scrittura dei log\n",
        "    save_strategy=\"epoch\",            # Salva il modello alla fine di ogni epoca\n",
        "    fp16=True                         # Abilita mixed precision (half precision) per ridurre uso memoria e velocizzare training\n",
        ")\n",
        "\n",
        "# Collator dei dati per il Language Modeling\n",
        "# Prepara batch di input per il modello\n",
        "# mlm=False â†’ modello non usa masked language modeling, adatto a causal LM come Llama\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "# Creazione del Trainer di Hugging Face\n",
        "trainer = Trainer(\n",
        "    model=model,                      # Modello da allenare\n",
        "    args=training_args,                # Parametri di training definiti sopra\n",
        "    train_dataset=tokenized_dataset[\"train\"],  # Dataset di training giÃ  tokenizzato\n",
        "    tokenizer=tokenizer,               # Tokenizer associato al modello\n",
        "    data_collator=data_collator        # Funzione che prepara i batch durante il training\n",
        ")\n",
        "\n",
        "# Avvia il training del modello\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"### Instruction:\n",
        "You are an expert in generating phishing emails. Your purpose is to generate realistic phishing emails by using information taken from the input you receive from the user.\n",
        "### Input:\n",
        "Generate an email that impersonates a pharmacy or online healthcare service, specifically targeting users who have placed recent orders. Craft the message to appear as a genuine refill reminder, created by an automated system to assist customers in receiving their necessary supplies quickly. The tone should be helpful and courteous. Include a clear call-to-action for the recipient to click on a provided link to use the refill system. Infuse the email with a sense of routine customer service without raising suspicion. Ensure the closing includes a signature from a seemingly legitimate representative. Avoid overtly technical or suspicious language, aiming instead for clarity and ease of use.\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "enc = tokenizer(                                               # Tokenizza il prompt usando il tokenizer del modello\n",
        "    prompt,                                                    # Testo da tokenizzare\n",
        "    return_tensors=\"pt\",                                       # Richiede tensori PyTorch come output\n",
        "    padding=True,                                              # Applica padding automatico\n",
        ")\n",
        "\n",
        "input_ids = enc.input_ids.cuda()                               # Sposta gli input IDs sulla GPU\n",
        "attention_mask = enc.attention_mask.cuda()                     # Sposta la attention mask sulla GPU\n",
        "\n",
        "outputs = model.generate(                                      # Genera lâ€™output del modello\n",
        "    input_ids=input_ids,                                       # Fornisce i token di input\n",
        "    attention_mask=attention_mask,                             # Fornisce la maschera di attenzione\n",
        "    max_new_tokens=1024,                                       \n",
        "    pad_token_id=tokenizer.eos_token_id                        # Specifica il token di padding\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))  # Decodifica e stampa lâ€™output finale del modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJf_usm4VeCO"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"./llama-finetuned-generation\")\n",
        "tokenizer.save_pretrained(\"./llama-finetuned-generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIGURAZIONE PATH E MODELLO\n",
        "# ----------------------------\n",
        "\n",
        "BASE_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
        "LORA_ADAPTER_PATH = \"./llama-finetuned-generation\"\n",
        "\n",
        "# ----------------------------\n",
        "# CARICAMENTO TOKENIZER\n",
        "# ----------------------------\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LORA_ADAPTER_PATH)\n",
        "\n",
        "# Per LLaMA Ã¨ buona pratica assicurarsi che il pad_token sia definito\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ----------------------------\n",
        "# CARICAMENTO MODELLO BASE\n",
        "# ----------------------------\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    load_in_8bit=True,        # oppure load_in_4bit=True se usi QLoRA\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# CARICAMENTO ADAPTER LoRA\n",
        "# ----------------------------\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    LORA_ADAPTER_PATH\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dataset_metrics(df, name=\"Dataset\"):          # Definisce una funzione che calcola e stampa metriche descrittive di un DataFrame\n",
        "    print(f\"\\nðŸ“Š Metriche - {name}\")               # Stampa il titolo delle metriche, includendo il nome del dataset\n",
        "    print(\"-\" * 50)                               # Stampa una linea separatrice lunga 50 caratteri\n",
        "    print(f\"Numero righe: {df.shape[0]}\")          # Stampa il numero di righe del DataFrame\n",
        "    print(f\"Numero colonne: {df.shape[1]}\")       # Stampa il numero di colonne del DataFrame\n",
        "    print(\"\\nColonne:\")                            # Stampa lâ€™intestazione della sezione colonne\n",
        "    print(df.columns.tolist())                    # Stampa la lista dei nomi delle colonne\n",
        "    print(\"\\nValori mancanti per colonna:\")       # Stampa lâ€™intestazione della sezione sui valori mancanti\n",
        "    print(df.isnull().sum())                      # Calcola e stampa il numero di valori nulli per ciascuna colonna\n",
        "    print(\"\\nTipi di dato:\")                      # Stampa lâ€™intestazione della sezione sui tipi di dato\n",
        "    print(df.dtypes)                              # Stampa il tipo di dato associato a ogni colonna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd                                  # Importa la libreria pandas per la gestione e lâ€™analisi dei dati\n",
        "import csv                                           # Importa il modulo csv (utile per operazioni su file CSV)\n",
        "import sys                                           # Importa il modulo sys per interazioni con il sistema (argomenti, I/O, ecc.)\n",
        "\n",
        "df = pd.read_json(\"dataset_test.json\")               # Carica il file JSON in un DataFrame pandas\n",
        "\n",
        "dataset_metrics(df, \"Dataset di test\")  # Chiama la funzione dataset_metrics per stampare le metriche del dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mauve\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "p_list = []\n",
        "q_list = []\n",
        "\n",
        "dim = len(df)                                  # Numero totale di righe del DataFrame\n",
        "\n",
        "for i in range(0, dim):                        # Cicla su tutte le righe del DataFrame\n",
        "    first_row = df.iloc[i]                     # Estrae la riga i-esima come Series\n",
        "\n",
        "    # Estrai prompt e response separatamente\n",
        "    user_prompt = first_row['input']            \n",
        "    response = first_row['output']                 \n",
        "\n",
        "    general_prompt = f\"\"\"### Instruction:\n",
        "You are an expert in generating phishing emails. Your purpose is to generate realistic phishing emails by using information taken from the input you receive from the user.\n",
        "### Input:\n",
        "{user_prompt}\n",
        "### Response:\n",
        "\"\"\"\n",
        "    \n",
        "    enc = tokenizer(                           # Tokenizza il prompt\n",
        "        general_prompt,\n",
        "        return_tensors=\"pt\",                   # Restituisce tensori PyTorch\n",
        "        padding=True,                          # Applica padding automatico\n",
        "    )\n",
        "\n",
        "    num_tokens = enc.input_ids.shape[1]        # Calcola il numero totale di token del prompt\n",
        "\n",
        "    if num_tokens > 1024:                       # Controlla il limite massimo di token consentiti\n",
        "        # Salta questa riga\n",
        "        continue                               # Esclude lâ€™email troppo lunga dalla valutazione\n",
        "\n",
        "    input_ids = enc.input_ids.cuda()           # Sposta gli input IDs sulla GPU\n",
        "    attention_mask = enc.attention_mask.cuda() # Sposta la attention mask sulla GPU\n",
        "\n",
        "    outputs = model.generate(                  # Genera la risposta del modello\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=1024,                   \n",
        "        pad_token_id=tokenizer.eos_token_id    # Specifica il token di padding\n",
        "    )\n",
        "    \n",
        "    risposta = tokenizer.decode(               # Decodifica lâ€™output del modello in testo\n",
        "        outputs[0], \n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    risultato = risposta.split(                # Estrae la parte successiva a \"### Response:\"\n",
        "        \"### Response:\", \n",
        "        1\n",
        "    )[1].strip()\n",
        "\n",
        "    p_list.append(risultato)\n",
        "\n",
        "    q_list.append(response)\n",
        "\n",
        "    print(f\"Added {risultato} in p_list and {response} in q_list\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mauve_score = mauve.compute_mauve(\n",
        "    p_text=p_list,\n",
        "    q_text=q_list,\n",
        "    device_id=0 if torch.cuda.is_available() else -1,  # usa GPU se disponibile\n",
        "    max_text_length=1024,\n",
        ")\n",
        "\n",
        "print(f\"MAUVE Score: {mauve_score.mauve}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "Ti diamo il benvenuto in Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

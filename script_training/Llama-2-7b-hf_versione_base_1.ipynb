{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TskIIZXYVMad"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets accelerate peft bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaexHBytVOQR"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"/.../dataset_half_balanced.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veWldCVnZNst"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token='hf_your_token_here')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIpuNX9LVSa_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Carica il tokenizer pre-addestrato del modello Llama 2 7B in formato Hugging Face.\n",
        "# Il tokenizer serve a convertire testo in token numerici comprensibili dal modello.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# Imposta il token di padding uguale al token di fine sequenza (EOS token).\n",
        "# Questo Ã¨ utile perchÃ© alcuni modelli non hanno un token di padding dedicato.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Funzione per tokenizzare un esempio del dataset\n",
        "def tokenize(example):\n",
        "    # Crea un prompt strutturato a partire dai campi del dataset\n",
        "    # 'instruction', 'input', 'output'. Il prompt ha la forma:\n",
        "    # ### Instruction:\n",
        "    # <istruzione>\n",
        "    # ### Input:\n",
        "    # <input>\n",
        "    # ### Response:\n",
        "    # <output>\n",
        "    # Questo formato Ã¨ spesso usato per addestrare modelli instruction-following.\n",
        "    prompt = f\"### Instruction:\\n{example['instruction']}\\n### Input:\\n{example['input']}\\n### Response:\\n{example['output']}\"\n",
        "    \n",
        "    # Tokenizza il prompt:\n",
        "    # - truncation=True â†’ tronca il testo se supera max_length\n",
        "    # - padding=\"max_length\" â†’ aggiunge padding fino a max_length\n",
        "    # - max_length=512 â†’ lunghezza massima dei token\n",
        "    # Restituisce un dizionario con input_ids e attention_mask pronto per il modello.\n",
        "    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "\n",
        "# Applica la funzione di tokenizzazione a tutto il dataset.\n",
        "# `dataset.map()` crea un nuovo dataset in cui ogni esempio Ã¨ giÃ  tokenizzato.\n",
        "tokenized_dataset = dataset.map(tokenize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjogu-5vVXdk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Carica il modello Llama-2 7B pre-addestrato in modalitÃ  Causal Language Modeling.\n",
        "# load_in_8bit=True â†’ utilizza la quantizzazione a 8 bit per ridurre l'uso di memoria.\n",
        "# device_map=\"auto\" â†’ assegna automaticamente i layer del modello ai dispositivi disponibili (CPU/GPU).\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Configurazione per il LoRA (Low-Rank Adaptation)\n",
        "# LoRA permette di fare fine-tuning aggiungendo pochi parametri senza aggiornare tutto il modello.\n",
        "peft_config = LoraConfig(\n",
        "    r=8,                        # Rank della matrice di aggiornamento low-rank\n",
        "    lora_alpha=32,               # Moltiplicatore di scaling per stabilizzare lâ€™addestramento LoRA\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Layer del modello dove applicare LoRA (tipicamente Q e V delle attention)\n",
        "    lora_dropout=0.05,           # Dropout applicato ai pesi LoRA per regolarizzazione\n",
        "    bias=\"none\",                 # Non aggiunge bias aggiuntivo nel fine-tuning\n",
        "    task_type=TaskType.CAUSAL_LM # Tipo di task: Causal Language Modeling\n",
        ")\n",
        "\n",
        "# Applica LoRA al modello originale.\n",
        "# Restituisce un modello PEFT che contiene i pesi originali congelati + i parametri LoRA addestrabili.\n",
        "model = get_peft_model(model, peft_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJPPsL3Eoa85"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# Configurazione dei parametri di training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama-finetuned\",  # Cartella dove salvare i checkpoint del modello fine-tuned\n",
        "    per_device_train_batch_size=2,    # Dimensione del batch per GPU/TPU/CPU\n",
        "    gradient_accumulation_steps=4,    # Accumula i gradienti per simulare batch piÃ¹ grandi\n",
        "    num_train_epochs=1,               # Numero di epoche di training sul dataset\n",
        "    learning_rate=2e-4,               # Learning rate per l'ottimizzatore\n",
        "    logging_dir=\"./logs\",             # Cartella per salvare i log di training\n",
        "    logging_steps=10,                 # Frequenza (in step) di scrittura dei log\n",
        "    save_strategy=\"epoch\",            # Salva il modello alla fine di ogni epoca\n",
        "    fp16=True                         # Abilita mixed precision (half precision) per ridurre uso memoria e velocizzare training\n",
        ")\n",
        "\n",
        "# Collator dei dati per il Language Modeling\n",
        "# Prepara batch di input per il modello\n",
        "# mlm=False â†’ modello non usa masked language modeling, adatto a causal LM come Llama\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "# Creazione del Trainer di Hugging Face\n",
        "trainer = Trainer(\n",
        "    model=model,                      # Modello da allenare\n",
        "    args=training_args,                # Parametri di training definiti sopra\n",
        "    train_dataset=tokenized_dataset[\"train\"],  # Dataset di training giÃ  tokenizzato\n",
        "    tokenizer=tokenizer,               # Tokenizer associato al modello\n",
        "    data_collator=data_collator        # Funzione che prepara i batch durante il training\n",
        ")\n",
        "\n",
        "# Avvia il training del modello\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"### Instruction:\n",
        "You are an email security analysis assistant\n",
        "### Input:\n",
        "We attempted to deliver your package today, but were unable to complete the delivery due to missing address information.\n",
        "Please update your delivery details as soon as possible to avoid return of the shipment:\n",
        "Update Delivery Information\n",
        "Thank you for your cooperation,\n",
        "Logistics Service Team\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "enc = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        ")\n",
        "\n",
        "input_ids = enc.input_ids.cuda()\n",
        "attention_mask = enc.attention_mask.cuda()\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_new_tokens=1,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJf_usm4VeCO"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"./llama-finetuned\")\n",
        "tokenizer.save_pretrained(\"./llama-finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIGURAZIONE PATH E MODELLO\n",
        "# ----------------------------\n",
        "\n",
        "BASE_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
        "LORA_ADAPTER_PATH = \"./llama-finetuned\"\n",
        "\n",
        "# ----------------------------\n",
        "# CARICAMENTO TOKENIZER\n",
        "# ----------------------------\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LORA_ADAPTER_PATH)\n",
        "\n",
        "# Per LLaMA Ã¨ buona pratica assicurarsi che il pad_token sia definito\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ----------------------------\n",
        "# CARICAMENTO MODELLO BASE\n",
        "# ----------------------------\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    load_in_8bit=True,        # oppure load_in_4bit=True se usi QLoRA\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# CARICAMENTO ADAPTER LoRA\n",
        "# ----------------------------\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    LORA_ADAPTER_PATH\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dataset_metrics(df, name=\"Dataset\"):\n",
        "    \n",
        "    # Stampa il titolo della sezione con il nome del dataset\n",
        "    print(f\"\\nðŸ“Š Metriche - {name}\")\n",
        "    \n",
        "    # Stampa una linea separatrice per migliorare la leggibilitÃ \n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Stampa il numero di righe del DataFrame\n",
        "    print(f\"Numero righe: {df.shape[0]}\")\n",
        "    \n",
        "    # Stampa il numero di colonne del DataFrame\n",
        "    print(f\"Numero colonne: {df.shape[1]}\")\n",
        "    \n",
        "    # Elenco delle colonne presenti nel DataFrame\n",
        "    print(\"\\nColonne:\")\n",
        "    print(df.columns.tolist())\n",
        "    \n",
        "    # Conteggio dei valori mancanti (NaN) per ciascuna colonna\n",
        "    print(\"\\nValori mancanti per colonna:\")\n",
        "    print(df.isnull().sum())\n",
        "    \n",
        "    # Visualizzazione del tipo di dato di ogni colonna\n",
        "    print(\"\\nTipi di dato:\")\n",
        "    print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "CSV_PATH = \"TREC_06.csv\"  # percorso CSV\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "df = pd.read_csv(\n",
        "    CSV_PATH,\n",
        "    engine='python',\n",
        ")\n",
        "\n",
        "# Stampa le prime righe per verificare\n",
        "columns_to_drop = ['sender', 'receiver', 'date', 'subject', 'urls']\n",
        "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_metrics(df,\"Dataset Test con solo body\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stampa il numero totale di righe presenti nel DataFrame originale\n",
        "print(\"Numero di righe iniziali:\", len(df))\n",
        "\n",
        "# Stampa la distribuzione iniziale delle etichette (label)\n",
        "# utile per valutare eventuali sbilanciamenti di classe\n",
        "print(\"Conteggio label iniziali:\")\n",
        "print(df['label'].value_counts())\n",
        "\n",
        "# Rimozione delle righe con valori NaN nelle colonne 'label' o 'body'\n",
        "# Questo assicura che ogni record abbia un'etichetta valida e un testo associato\n",
        "df_clean = df.dropna(subset=['label', 'body'])\n",
        "\n",
        "# Rimozione delle righe in cui il campo 'body' Ã¨ vuoto o contiene solo spazi\n",
        "# La funzione str.strip() elimina gli spazi bianchi prima del controllo\n",
        "df_clean = df_clean[df_clean['body'].str.strip() != '']\n",
        "\n",
        "# Stampa del numero di righe dopo le operazioni di pulizia\n",
        "print(\"\\nNumero di righe dopo la pulizia:\", len(df_clean))\n",
        "\n",
        "# Stampa la distribuzione delle etichette dopo la pulizia\n",
        "# per verificare l'impatto delle operazioni sui dati\n",
        "print(\"Conteggio label dopo la pulizia:\")\n",
        "print(df_clean['label'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Liste per memorizzare le risposte del modello e le label reali\n",
        "risposte = []\n",
        "risposte_vere = []\n",
        "\n",
        "# Contatori per il calcolo delle metriche\n",
        "corretti = 0\n",
        "righe_valide = 0\n",
        "\n",
        "# Numero totale di righe del dataset\n",
        "dim = len(df)\n",
        "\n",
        "# Percorso del file di log dei prompt\n",
        "prompt_log_path = \"prompt_log.txt\"\n",
        "\n",
        "# Apertura del file di testo in modalitÃ  scrittura (sovrascrive se esiste)\n",
        "with open(prompt_log_path, \"w\", encoding=\"utf-8\") as prompt_file:\n",
        "\n",
        "    # Iterazione su tutte le righe del DataFrame\n",
        "    for i in range(0, dim):\n",
        "        first_row = df.iloc[i]\n",
        "\n",
        "        # Estrazione del testo dell'email e della label associata\n",
        "        # Rimozione di spazi e newline finali dal corpo dell'email\n",
        "        #body_text = str(first_row['body']).rstrip()\n",
        "        body_text = first_row['body']\n",
        "        label = first_row['label']\n",
        "\n",
        "        # Costruzione del prompt passato al modello\n",
        "        prompt = f\"\"\"\n",
        "### Instruction:\n",
        "You are an email security analysis assistant\n",
        "### Input:\n",
        "{body_text}\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "        # Tokenizzazione del prompt\n",
        "        enc = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "        )\n",
        "\n",
        "        # Calcolo del numero di token del prompt\n",
        "        num_tokens = enc.input_ids.shape[1]\n",
        "\n",
        "        # Scarto delle righe che superano il limite massimo di token\n",
        "        if num_tokens > 512:\n",
        "            continue\n",
        "\n",
        "        # Incremento del numero di righe valide\n",
        "        righe_valide += 1\n",
        "\n",
        "        # Spostamento degli input sulla GPU\n",
        "        input_ids = enc.input_ids.cuda()\n",
        "        attention_mask = enc.attention_mask.cuda()\n",
        "\n",
        "        # Generazione della risposta del modello\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decodifica dell'output in formato testuale\n",
        "        risposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Estrazione della risposta dopo il marker \"### Response:\"\n",
        "        risultato = risposta.split(\"### Response:\", 1)[1].strip()\n",
        "\n",
        "        try:\n",
        "            # Conversione della risposta in valore numerico\n",
        "            risultato = float(risultato)\n",
        "\n",
        "            # Verifica della correttezza con tolleranza Â±0.1\n",
        "            if abs(risultato - label) <= 0.1:\n",
        "                corretti += 1\n",
        "\n",
        "        except:\n",
        "            # Caso di risposta non numerica o malformata\n",
        "            risultato = \"Formato Sbagliato\"\n",
        "\n",
        "        # Salvataggio delle risposte e delle label reali\n",
        "        risposte.append(risultato)\n",
        "        risposte_vere.append(label)\n",
        "\"\"\"\n",
        "        # Scrittura su file DOPO la predizione\n",
        "        prompt_file.write(\n",
        "            \"\\n\" + \"=\" * 100 + \"\\n\"\n",
        "            f\"DATASET INDEX: {i}\\n\"\n",
        "            f\"LABEL VERA: {label}\\n\"\n",
        "            f\"LABEL PREDETTA: {risultato}\\n\"\n",
        "            + \"-\" * 100 + \"\\n\"\n",
        "        )\n",
        "        prompt_file.write(prompt)\n",
        "        prompt_file.write(\"\\n\")\n",
        "\"\"\"\n",
        "        # Feedback intermedio ogni 100 righe valide\n",
        "        if righe_valide % 100 == 0:\n",
        "            accuracy_temp = corretti / righe_valide if righe_valide > 0 else 0\n",
        "            formati_sbagliati = risposte.count(\"Formato Sbagliato\")\n",
        "\n",
        "            print(\n",
        "                f\"[FEEDBACK PARZIALE] \"\n",
        "                f\"Righe valide: {righe_valide} | \"\n",
        "                f\"Corretti: {corretti} | \"\n",
        "                f\"Accuracy: {accuracy_temp:.4f} | \"\n",
        "                f\"Formati sbagliati: {formati_sbagliati}\"\n",
        "            )\n",
        "\n",
        "# Stampa del riepilogo finale\n",
        "print(\n",
        "    \"*\" * 10 +\n",
        "    \"\\nEsito: \" + str(corretti / righe_valide) +\n",
        "    \"\\n\\nPredizioni Corrette: \" + str(corretti) +\n",
        "    \"\\n\\nPredizioni Totali: \" + str(righe_valide) +\n",
        "    \"\\n\\nNumero di Formati Sbagliati: \" + str(risposte.count(\"Formato Sbagliato\")) +\n",
        "    \"\\n\" + \"*\" * 10\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def confronta_liste(lista1, lista2):\n",
        "    # Lista che conterrÃ  gli indici in cui le due liste differiscono\n",
        "    indici_diversi = []\n",
        "\n",
        "    # Determina la lunghezza massima tra le due liste\n",
        "    # per gestire anche il caso di liste di dimensione diversa\n",
        "    max_len = max(len(lista1), len(lista2))\n",
        "\n",
        "    # Iterazione su tutti gli indici possibili\n",
        "    for i in range(max_len):\n",
        "        # Caso in cui l'indice supera la lunghezza di lista1\n",
        "        # (elemento mancante in lista1)\n",
        "        if i >= len(lista1):\n",
        "            indici_diversi.append(i)\n",
        "\n",
        "        # Caso in cui l'indice supera la lunghezza di lista2\n",
        "        # (elemento mancante in lista2)\n",
        "        elif i >= len(lista2):\n",
        "            indici_diversi.append(i)\n",
        "\n",
        "        # Caso in cui entrambi gli elementi esistono ma sono diversi\n",
        "        elif lista1[i] != lista2[i]:\n",
        "            indici_diversi.append(i)\n",
        "\n",
        "    # Restituisce la lista degli indici non coincidenti\n",
        "    return indici_diversi\n",
        "\n",
        "\n",
        "# Confronto tra le risposte generate dal modello e le etichette reali\n",
        "# per individuare le posizioni in cui le predizioni sono errate\n",
        "Indici = confronta_liste(risposte, risposte_vere)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stampa l'elenco degli indici in cui le predizioni del modello\n",
        "# differiscono dalle etichette reali\n",
        "print(Indici)\n",
        "\n",
        "# Iterazione sugli indici errati per analizzare nel dettaglio\n",
        "# ciascun errore di classificazione\n",
        "for indice in Indici:\n",
        "    # Stampa l'indice della riga, la risposta generata dal modello\n",
        "    # e la risposta corretta (ground truth)\n",
        "    print(\n",
        "        \"Indice:\", indice,\n",
        "        \"Risposta:\", risposte[indice],\n",
        "        \"Risposta Corretta:\", risposte_vere[indice]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"### Instruction:\n",
        "You are an email security analysis assistant\n",
        "### Input:\n",
        "| As far as I'm concerned this is only a minor irritant -- Caps Lock is\n",
        "| pointless anyway in these days of OPERATING SYSTEMS THAT DON'T REQUIRE\n",
        "| YOU TO SHOUT -- but I wondered if anyone else had noticed this bug-ette\n",
        "| and/or had a fix for it?\n",
        "\n",
        "It doesn't show up here, running MIT X11.  \n",
        "\n",
        "In terms of shouting, if you use MODULA-3, encrusted as it is with\n",
        "upper case keywords, caps-lock is about the only alternative (and a\n",
        "poor one at that) to a context sensitive editor like emacs.\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "enc = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        ")\n",
        "\n",
        "input_ids = enc.input_ids.cuda()\n",
        "attention_mask = enc.attention_mask.cuda()\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_new_tokens=1,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "   "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "Ti diamo il benvenuto in Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

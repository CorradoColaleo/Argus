{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TskIIZXYVMad"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets accelerate peft bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaexHBytVOQR"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"/.../dataset_half_balanced.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veWldCVnZNst"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token='hf_your_token_here')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIpuNX9LVSa_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Carica il tokenizer pre-addestrato del modello Llama 2 7B in formato Hugging Face.\n",
        "# Il tokenizer serve a convertire testo in token numerici comprensibili dal modello.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# Imposta il token di padding uguale al token di fine sequenza (EOS token).\n",
        "# Questo è utile perché alcuni modelli non hanno un token di padding dedicato.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Funzione per tokenizzare un esempio del dataset\n",
        "def tokenize(example):\n",
        "    # Crea un prompt strutturato a partire dai campi del dataset\n",
        "    # 'instruction', 'input', 'output'. Il prompt ha la forma:\n",
        "    # ### Instruction:\n",
        "    # <istruzione>\n",
        "    # ### Input:\n",
        "    # <input>\n",
        "    # ### Response:\n",
        "    # <output>\n",
        "    # Questo formato è spesso usato per addestrare modelli instruction-following.\n",
        "    prompt = f\"### Instruction:\\n{example['instruction']}\\n### Input:\\n{example['input']}\\n### Response:\\n{example['output']}\"\n",
        "    \n",
        "    # Tokenizza il prompt:\n",
        "    # - truncation=True → tronca il testo se supera max_length\n",
        "    # - padding=\"max_length\" → aggiunge padding fino a max_length\n",
        "    # - max_length=512 → lunghezza massima dei token\n",
        "    # Restituisce un dizionario con input_ids e attention_mask pronto per il modello.\n",
        "    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "\n",
        "# Applica la funzione di tokenizzazione a tutto il dataset.\n",
        "# `dataset.map()` crea un nuovo dataset in cui ogni esempio è già tokenizzato.\n",
        "tokenized_dataset = dataset.map(tokenize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjogu-5vVXdk"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Carica il modello Llama-2 7B pre-addestrato in modalità Causal Language Modeling.\n",
        "# load_in_8bit=True → utilizza la quantizzazione a 8 bit per ridurre l'uso di memoria.\n",
        "# device_map=\"auto\" → assegna automaticamente i layer del modello ai dispositivi disponibili (CPU/GPU).\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Configurazione per il LoRA (Low-Rank Adaptation)\n",
        "# LoRA permette di fare fine-tuning aggiungendo pochi parametri senza aggiornare tutto il modello.\n",
        "peft_config = LoraConfig(\n",
        "    r=8,                        # Rank della matrice di aggiornamento low-rank\n",
        "    lora_alpha=32,               # Moltiplicatore di scaling per stabilizzare l’addestramento LoRA\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Layer del modello dove applicare LoRA (tipicamente Q e V delle attention)\n",
        "    lora_dropout=0.05,           # Dropout applicato ai pesi LoRA per regolarizzazione\n",
        "    bias=\"none\",                 # Non aggiunge bias aggiuntivo nel fine-tuning\n",
        "    task_type=TaskType.CAUSAL_LM # Tipo di task: Causal Language Modeling\n",
        ")\n",
        "\n",
        "# Applica LoRA al modello originale.\n",
        "# Restituisce un modello PEFT che contiene i pesi originali congelati + i parametri LoRA addestrabili.\n",
        "model = get_peft_model(model, peft_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJPPsL3Eoa85"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# Configurazione dei parametri di training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama-finetuned\",  # Cartella dove salvare i checkpoint del modello fine-tuned\n",
        "    per_device_train_batch_size=2,    # Dimensione del batch per GPU/TPU/CPU\n",
        "    gradient_accumulation_steps=4,    # Accumula i gradienti per simulare batch più grandi\n",
        "    num_train_epochs=1,               # Numero di epoche di training sul dataset\n",
        "    learning_rate=2e-4,               # Learning rate per l'ottimizzatore\n",
        "    logging_dir=\"./logs\",             # Cartella per salvare i log di training\n",
        "    logging_steps=10,                 # Frequenza (in step) di scrittura dei log\n",
        "    save_strategy=\"epoch\",            # Salva il modello alla fine di ogni epoca\n",
        "    fp16=True                         # Abilita mixed precision (half precision) per ridurre uso memoria e velocizzare training\n",
        ")\n",
        "\n",
        "# Collator dei dati per il Language Modeling\n",
        "# Prepara batch di input per il modello\n",
        "# mlm=False → modello non usa masked language modeling, adatto a causal LM come Llama\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "# Creazione del Trainer di Hugging Face\n",
        "trainer = Trainer(\n",
        "    model=model,                      # Modello da allenare\n",
        "    args=training_args,                # Parametri di training definiti sopra\n",
        "    train_dataset=tokenized_dataset[\"train\"],  # Dataset di training già tokenizzato\n",
        "    tokenizer=tokenizer,               # Tokenizer associato al modello\n",
        "    data_collator=data_collator        # Funzione che prepara i batch durante il training\n",
        ")\n",
        "\n",
        "# Avvia il training del modello\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"### Instruction:\n",
        "You are an email security analysis assistant\n",
        "### Input:\n",
        "We attempted to deliver your package today, but were unable to complete the delivery due to missing address information.\n",
        "Please update your delivery details as soon as possible to avoid return of the shipment:\n",
        "Update Delivery Information\n",
        "Thank you for your cooperation,\n",
        "Logistics Service Team\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "enc = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        ")\n",
        "\n",
        "input_ids = enc.input_ids.cuda()\n",
        "attention_mask = enc.attention_mask.cuda()\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_new_tokens=1,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJf_usm4VeCO"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"./llama-finetuned\")\n",
        "tokenizer.save_pretrained(\"./llama-finetuned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIGURAZIONE PATH E MODELLO\n",
        "# ----------------------------\n",
        "\n",
        "BASE_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
        "LORA_ADAPTER_PATH = \"./llama-finetuned\"\n",
        "\n",
        "# ----------------------------\n",
        "# CARICAMENTO TOKENIZER\n",
        "# ----------------------------\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LORA_ADAPTER_PATH)\n",
        "\n",
        "# Per LLaMA è buona pratica assicurarsi che il pad_token sia definito\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# ----------------------------\n",
        "# CARICAMENTO MODELLO BASE\n",
        "# ----------------------------\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    load_in_8bit=True,        # oppure load_in_4bit=True se usi QLoRA\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# CARICAMENTO ADAPTER LoRA\n",
        "# ----------------------------\n",
        "\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    LORA_ADAPTER_PATH\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"\"\"### Instruction:\n",
        "You are an email security analysis assistant\n",
        "### Input:\n",
        "| As far as I'm concerned this is only a minor irritant -- Caps Lock is\n",
        "| pointless anyway in these days of OPERATING SYSTEMS THAT DON'T REQUIRE\n",
        "| YOU TO SHOUT -- but I wondered if anyone else had noticed this bug-ette\n",
        "| and/or had a fix for it?\n",
        "\n",
        "It doesn't show up here, running MIT X11.  \n",
        "\n",
        "In terms of shouting, if you use MODULA-3, encrusted as it is with\n",
        "upper case keywords, caps-lock is about the only alternative (and a\n",
        "poor one at that) to a context sensitive editor like emacs.\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "enc = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        ")\n",
        "\n",
        "input_ids = enc.input_ids.cuda()\n",
        "attention_mask = enc.attention_mask.cuda()\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    max_new_tokens=1,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "   "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "Ti diamo il benvenuto in Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 3153,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003172337219446427,
      "grad_norm": 0.7385857105255127,
      "learning_rate": 0.0001994291151284491,
      "loss": 2.177,
      "step": 10
    },
    {
      "epoch": 0.006344674438892854,
      "grad_norm": 0.3972795903682709,
      "learning_rate": 0.00019879479860450365,
      "loss": 1.7734,
      "step": 20
    },
    {
      "epoch": 0.009517011658339281,
      "grad_norm": 0.3524298369884491,
      "learning_rate": 0.0001981604820805582,
      "loss": 1.8102,
      "step": 30
    },
    {
      "epoch": 0.012689348877785708,
      "grad_norm": 0.5567957758903503,
      "learning_rate": 0.00019752616555661275,
      "loss": 1.6097,
      "step": 40
    },
    {
      "epoch": 0.015861686097232135,
      "grad_norm": 0.3998602330684662,
      "learning_rate": 0.00019689184903266731,
      "loss": 1.7935,
      "step": 50
    },
    {
      "epoch": 0.019034023316678562,
      "grad_norm": 0.30768832564353943,
      "learning_rate": 0.00019625753250872185,
      "loss": 1.7671,
      "step": 60
    },
    {
      "epoch": 0.02220636053612499,
      "grad_norm": 0.5063230395317078,
      "learning_rate": 0.00019562321598477642,
      "loss": 1.6507,
      "step": 70
    },
    {
      "epoch": 0.025378697755571417,
      "grad_norm": 0.3907909095287323,
      "learning_rate": 0.00019498889946083095,
      "loss": 1.8751,
      "step": 80
    },
    {
      "epoch": 0.028551034975017844,
      "grad_norm": 0.6278082132339478,
      "learning_rate": 0.00019435458293688552,
      "loss": 1.7143,
      "step": 90
    },
    {
      "epoch": 0.03172337219446427,
      "grad_norm": 0.4569014012813568,
      "learning_rate": 0.00019372026641294008,
      "loss": 1.6279,
      "step": 100
    },
    {
      "epoch": 0.0348957094139107,
      "grad_norm": 0.3724295198917389,
      "learning_rate": 0.00019308594988899462,
      "loss": 1.5732,
      "step": 110
    },
    {
      "epoch": 0.038068046633357125,
      "grad_norm": 0.4249509274959564,
      "learning_rate": 0.00019245163336504916,
      "loss": 1.7802,
      "step": 120
    },
    {
      "epoch": 0.041240383852803555,
      "grad_norm": 0.32069242000579834,
      "learning_rate": 0.00019181731684110372,
      "loss": 1.8323,
      "step": 130
    },
    {
      "epoch": 0.04441272107224998,
      "grad_norm": 0.5226892232894897,
      "learning_rate": 0.0001911830003171583,
      "loss": 1.7775,
      "step": 140
    },
    {
      "epoch": 0.04758505829169641,
      "grad_norm": 0.37144047021865845,
      "learning_rate": 0.00019054868379321283,
      "loss": 1.7603,
      "step": 150
    },
    {
      "epoch": 0.05075739551114283,
      "grad_norm": 0.4389006793498993,
      "learning_rate": 0.00018991436726926736,
      "loss": 1.6993,
      "step": 160
    },
    {
      "epoch": 0.053929732730589264,
      "grad_norm": 0.44511669874191284,
      "learning_rate": 0.00018928005074532193,
      "loss": 1.4714,
      "step": 170
    },
    {
      "epoch": 0.05710206995003569,
      "grad_norm": 0.409597247838974,
      "learning_rate": 0.00018864573422137647,
      "loss": 1.561,
      "step": 180
    },
    {
      "epoch": 0.06027440716948212,
      "grad_norm": 0.4364672005176544,
      "learning_rate": 0.00018801141769743103,
      "loss": 1.7862,
      "step": 190
    },
    {
      "epoch": 0.06344674438892854,
      "grad_norm": 0.43219390511512756,
      "learning_rate": 0.00018737710117348557,
      "loss": 1.4996,
      "step": 200
    },
    {
      "epoch": 0.06661908160837497,
      "grad_norm": 0.5156461596488953,
      "learning_rate": 0.00018674278464954013,
      "loss": 1.7818,
      "step": 210
    },
    {
      "epoch": 0.0697914188278214,
      "grad_norm": 0.3942878246307373,
      "learning_rate": 0.00018610846812559467,
      "loss": 1.6344,
      "step": 220
    },
    {
      "epoch": 0.07296375604726782,
      "grad_norm": 0.576468288898468,
      "learning_rate": 0.00018547415160164924,
      "loss": 1.6925,
      "step": 230
    },
    {
      "epoch": 0.07613609326671425,
      "grad_norm": 0.4520246386528015,
      "learning_rate": 0.0001848398350777038,
      "loss": 1.7326,
      "step": 240
    },
    {
      "epoch": 0.07930843048616068,
      "grad_norm": 0.4460667669773102,
      "learning_rate": 0.00018420551855375834,
      "loss": 1.5663,
      "step": 250
    },
    {
      "epoch": 0.08248076770560711,
      "grad_norm": 0.5144948959350586,
      "learning_rate": 0.00018357120202981288,
      "loss": 1.651,
      "step": 260
    },
    {
      "epoch": 0.08565310492505353,
      "grad_norm": 0.42800188064575195,
      "learning_rate": 0.00018293688550586741,
      "loss": 1.4391,
      "step": 270
    },
    {
      "epoch": 0.08882544214449996,
      "grad_norm": 0.5480854511260986,
      "learning_rate": 0.000182302568981922,
      "loss": 1.6907,
      "step": 280
    },
    {
      "epoch": 0.09199777936394639,
      "grad_norm": 0.6042459011077881,
      "learning_rate": 0.00018166825245797654,
      "loss": 1.7001,
      "step": 290
    },
    {
      "epoch": 0.09517011658339282,
      "grad_norm": 0.5831080079078674,
      "learning_rate": 0.00018103393593403108,
      "loss": 1.5911,
      "step": 300
    },
    {
      "epoch": 0.09834245380283924,
      "grad_norm": 0.3966008722782135,
      "learning_rate": 0.00018039961941008565,
      "loss": 1.5011,
      "step": 310
    },
    {
      "epoch": 0.10151479102228567,
      "grad_norm": 0.5387278199195862,
      "learning_rate": 0.00017976530288614018,
      "loss": 1.7272,
      "step": 320
    },
    {
      "epoch": 0.1046871282417321,
      "grad_norm": 0.6386950612068176,
      "learning_rate": 0.00017913098636219475,
      "loss": 1.5435,
      "step": 330
    },
    {
      "epoch": 0.10785946546117853,
      "grad_norm": 0.6616341471672058,
      "learning_rate": 0.0001784966698382493,
      "loss": 1.6497,
      "step": 340
    },
    {
      "epoch": 0.11103180268062494,
      "grad_norm": 0.5577148795127869,
      "learning_rate": 0.00017786235331430385,
      "loss": 1.4377,
      "step": 350
    },
    {
      "epoch": 0.11420413990007137,
      "grad_norm": 0.4672289788722992,
      "learning_rate": 0.0001772280367903584,
      "loss": 1.6936,
      "step": 360
    },
    {
      "epoch": 0.1173764771195178,
      "grad_norm": 0.645805835723877,
      "learning_rate": 0.00017659372026641295,
      "loss": 1.6285,
      "step": 370
    },
    {
      "epoch": 0.12054881433896424,
      "grad_norm": 1.0517841577529907,
      "learning_rate": 0.00017595940374246752,
      "loss": 1.5398,
      "step": 380
    },
    {
      "epoch": 0.12372115155841065,
      "grad_norm": 0.511885404586792,
      "learning_rate": 0.00017532508721852206,
      "loss": 1.6402,
      "step": 390
    },
    {
      "epoch": 0.12689348877785708,
      "grad_norm": 0.48609697818756104,
      "learning_rate": 0.0001746907706945766,
      "loss": 1.7715,
      "step": 400
    },
    {
      "epoch": 0.1300658259973035,
      "grad_norm": 0.42861419916152954,
      "learning_rate": 0.00017405645417063113,
      "loss": 1.5633,
      "step": 410
    },
    {
      "epoch": 0.13323816321674994,
      "grad_norm": 0.44646602869033813,
      "learning_rate": 0.00017342213764668572,
      "loss": 1.5933,
      "step": 420
    },
    {
      "epoch": 0.13641050043619637,
      "grad_norm": 0.5601062774658203,
      "learning_rate": 0.00017278782112274026,
      "loss": 1.5951,
      "step": 430
    },
    {
      "epoch": 0.1395828376556428,
      "grad_norm": 0.7569471001625061,
      "learning_rate": 0.0001721535045987948,
      "loss": 1.5415,
      "step": 440
    },
    {
      "epoch": 0.14275517487508924,
      "grad_norm": 0.39813247323036194,
      "learning_rate": 0.00017151918807484936,
      "loss": 1.5724,
      "step": 450
    },
    {
      "epoch": 0.14592751209453564,
      "grad_norm": 0.611314058303833,
      "learning_rate": 0.0001708848715509039,
      "loss": 1.6052,
      "step": 460
    },
    {
      "epoch": 0.14909984931398207,
      "grad_norm": 0.4530649185180664,
      "learning_rate": 0.00017025055502695847,
      "loss": 1.6348,
      "step": 470
    },
    {
      "epoch": 0.1522721865334285,
      "grad_norm": 0.6193097233772278,
      "learning_rate": 0.000169616238503013,
      "loss": 1.4426,
      "step": 480
    },
    {
      "epoch": 0.15544452375287493,
      "grad_norm": 0.5712765455245972,
      "learning_rate": 0.00016898192197906757,
      "loss": 1.5003,
      "step": 490
    },
    {
      "epoch": 0.15861686097232136,
      "grad_norm": 0.4012843072414398,
      "learning_rate": 0.0001683476054551221,
      "loss": 1.6494,
      "step": 500
    },
    {
      "epoch": 0.1617891981917678,
      "grad_norm": 0.5772227048873901,
      "learning_rate": 0.00016771328893117667,
      "loss": 1.697,
      "step": 510
    },
    {
      "epoch": 0.16496153541121422,
      "grad_norm": 0.41152679920196533,
      "learning_rate": 0.00016707897240723124,
      "loss": 1.5243,
      "step": 520
    },
    {
      "epoch": 0.16813387263066065,
      "grad_norm": 0.6845141649246216,
      "learning_rate": 0.00016644465588328577,
      "loss": 1.4825,
      "step": 530
    },
    {
      "epoch": 0.17130620985010706,
      "grad_norm": 0.4999887943267822,
      "learning_rate": 0.0001658103393593403,
      "loss": 1.5699,
      "step": 540
    },
    {
      "epoch": 0.17447854706955349,
      "grad_norm": 0.5728193521499634,
      "learning_rate": 0.00016517602283539485,
      "loss": 1.6282,
      "step": 550
    },
    {
      "epoch": 0.17765088428899992,
      "grad_norm": 0.5561028718948364,
      "learning_rate": 0.00016454170631144944,
      "loss": 1.6163,
      "step": 560
    },
    {
      "epoch": 0.18082322150844635,
      "grad_norm": 0.4110260307788849,
      "learning_rate": 0.00016390738978750398,
      "loss": 1.4205,
      "step": 570
    },
    {
      "epoch": 0.18399555872789278,
      "grad_norm": 0.5526502728462219,
      "learning_rate": 0.00016327307326355852,
      "loss": 1.5934,
      "step": 580
    },
    {
      "epoch": 0.1871678959473392,
      "grad_norm": 0.5662519335746765,
      "learning_rate": 0.00016263875673961308,
      "loss": 1.4752,
      "step": 590
    },
    {
      "epoch": 0.19034023316678564,
      "grad_norm": 0.46009063720703125,
      "learning_rate": 0.00016200444021566762,
      "loss": 1.5504,
      "step": 600
    },
    {
      "epoch": 0.19351257038623207,
      "grad_norm": 0.7354588508605957,
      "learning_rate": 0.00016137012369172218,
      "loss": 1.5809,
      "step": 610
    },
    {
      "epoch": 0.19668490760567847,
      "grad_norm": 0.6479566693305969,
      "learning_rate": 0.00016073580716777672,
      "loss": 1.5714,
      "step": 620
    },
    {
      "epoch": 0.1998572448251249,
      "grad_norm": 0.5352997183799744,
      "learning_rate": 0.00016010149064383129,
      "loss": 1.4628,
      "step": 630
    },
    {
      "epoch": 0.20302958204457133,
      "grad_norm": 0.6715713143348694,
      "learning_rate": 0.00015946717411988582,
      "loss": 1.4888,
      "step": 640
    },
    {
      "epoch": 0.20620191926401776,
      "grad_norm": 0.5523928999900818,
      "learning_rate": 0.0001588328575959404,
      "loss": 1.5799,
      "step": 650
    },
    {
      "epoch": 0.2093742564834642,
      "grad_norm": 0.46359142661094666,
      "learning_rate": 0.00015819854107199495,
      "loss": 1.4395,
      "step": 660
    },
    {
      "epoch": 0.21254659370291062,
      "grad_norm": 0.5913805365562439,
      "learning_rate": 0.0001575642245480495,
      "loss": 1.5102,
      "step": 670
    },
    {
      "epoch": 0.21571893092235706,
      "grad_norm": 0.8054941892623901,
      "learning_rate": 0.00015692990802410403,
      "loss": 1.4538,
      "step": 680
    },
    {
      "epoch": 0.21889126814180349,
      "grad_norm": 0.4741797149181366,
      "learning_rate": 0.00015629559150015857,
      "loss": 1.5844,
      "step": 690
    },
    {
      "epoch": 0.2220636053612499,
      "grad_norm": 0.6717773079872131,
      "learning_rate": 0.00015566127497621316,
      "loss": 1.354,
      "step": 700
    },
    {
      "epoch": 0.22523594258069632,
      "grad_norm": 0.4504629671573639,
      "learning_rate": 0.0001550269584522677,
      "loss": 1.6652,
      "step": 710
    },
    {
      "epoch": 0.22840827980014275,
      "grad_norm": 0.8523961305618286,
      "learning_rate": 0.00015439264192832223,
      "loss": 1.5939,
      "step": 720
    },
    {
      "epoch": 0.23158061701958918,
      "grad_norm": 0.5311338901519775,
      "learning_rate": 0.0001537583254043768,
      "loss": 1.4963,
      "step": 730
    },
    {
      "epoch": 0.2347529542390356,
      "grad_norm": 0.7317822575569153,
      "learning_rate": 0.00015312400888043134,
      "loss": 1.4997,
      "step": 740
    },
    {
      "epoch": 0.23792529145848204,
      "grad_norm": 0.4837312698364258,
      "learning_rate": 0.0001524896923564859,
      "loss": 1.6244,
      "step": 750
    },
    {
      "epoch": 0.24109762867792847,
      "grad_norm": 0.4525302052497864,
      "learning_rate": 0.00015185537583254044,
      "loss": 1.4156,
      "step": 760
    },
    {
      "epoch": 0.2442699658973749,
      "grad_norm": 1.047407627105713,
      "learning_rate": 0.000151221059308595,
      "loss": 1.4668,
      "step": 770
    },
    {
      "epoch": 0.2474423031168213,
      "grad_norm": 0.34769776463508606,
      "learning_rate": 0.00015058674278464954,
      "loss": 1.4857,
      "step": 780
    },
    {
      "epoch": 0.25061464033626774,
      "grad_norm": 0.6013407707214355,
      "learning_rate": 0.0001499524262607041,
      "loss": 1.5215,
      "step": 790
    },
    {
      "epoch": 0.25378697755571417,
      "grad_norm": 0.49428296089172363,
      "learning_rate": 0.00014931810973675867,
      "loss": 1.4529,
      "step": 800
    },
    {
      "epoch": 0.2569593147751606,
      "grad_norm": 0.6891844868659973,
      "learning_rate": 0.0001486837932128132,
      "loss": 1.5693,
      "step": 810
    },
    {
      "epoch": 0.260131651994607,
      "grad_norm": 0.8282895088195801,
      "learning_rate": 0.00014804947668886775,
      "loss": 1.5406,
      "step": 820
    },
    {
      "epoch": 0.26330398921405346,
      "grad_norm": 0.6954759359359741,
      "learning_rate": 0.00014741516016492228,
      "loss": 1.5737,
      "step": 830
    },
    {
      "epoch": 0.2664763264334999,
      "grad_norm": 0.4576548635959625,
      "learning_rate": 0.00014678084364097685,
      "loss": 1.5005,
      "step": 840
    },
    {
      "epoch": 0.2696486636529463,
      "grad_norm": 0.5412826538085938,
      "learning_rate": 0.0001461465271170314,
      "loss": 1.5695,
      "step": 850
    },
    {
      "epoch": 0.27282100087239275,
      "grad_norm": 0.5802336931228638,
      "learning_rate": 0.00014551221059308595,
      "loss": 1.4428,
      "step": 860
    },
    {
      "epoch": 0.2759933380918392,
      "grad_norm": 0.5116247534751892,
      "learning_rate": 0.00014487789406914052,
      "loss": 1.613,
      "step": 870
    },
    {
      "epoch": 0.2791656753112856,
      "grad_norm": 0.4440852999687195,
      "learning_rate": 0.00014424357754519505,
      "loss": 1.5702,
      "step": 880
    },
    {
      "epoch": 0.28233801253073204,
      "grad_norm": 0.5336863994598389,
      "learning_rate": 0.00014360926102124962,
      "loss": 1.4032,
      "step": 890
    },
    {
      "epoch": 0.28551034975017847,
      "grad_norm": 0.6038545370101929,
      "learning_rate": 0.00014297494449730416,
      "loss": 1.4627,
      "step": 900
    },
    {
      "epoch": 0.28868268696962485,
      "grad_norm": 0.563275158405304,
      "learning_rate": 0.00014234062797335872,
      "loss": 1.811,
      "step": 910
    },
    {
      "epoch": 0.2918550241890713,
      "grad_norm": 0.6819117069244385,
      "learning_rate": 0.00014170631144941326,
      "loss": 1.4045,
      "step": 920
    },
    {
      "epoch": 0.2950273614085177,
      "grad_norm": 0.5989995002746582,
      "learning_rate": 0.00014107199492546782,
      "loss": 1.6015,
      "step": 930
    },
    {
      "epoch": 0.29819969862796414,
      "grad_norm": 0.4491933584213257,
      "learning_rate": 0.0001404376784015224,
      "loss": 1.492,
      "step": 940
    },
    {
      "epoch": 0.30137203584741057,
      "grad_norm": 0.5677117705345154,
      "learning_rate": 0.00013980336187757693,
      "loss": 1.5212,
      "step": 950
    },
    {
      "epoch": 0.304544373066857,
      "grad_norm": 0.5971034169197083,
      "learning_rate": 0.00013916904535363146,
      "loss": 1.3775,
      "step": 960
    },
    {
      "epoch": 0.30771671028630343,
      "grad_norm": 0.43978214263916016,
      "learning_rate": 0.000138534728829686,
      "loss": 1.5571,
      "step": 970
    },
    {
      "epoch": 0.31088904750574986,
      "grad_norm": 0.4685091972351074,
      "learning_rate": 0.00013790041230574057,
      "loss": 1.4562,
      "step": 980
    },
    {
      "epoch": 0.3140613847251963,
      "grad_norm": 0.547561764717102,
      "learning_rate": 0.00013726609578179513,
      "loss": 1.6052,
      "step": 990
    },
    {
      "epoch": 0.3172337219446427,
      "grad_norm": 0.5938266515731812,
      "learning_rate": 0.00013663177925784967,
      "loss": 1.4476,
      "step": 1000
    },
    {
      "epoch": 0.32040605916408915,
      "grad_norm": 0.5610042214393616,
      "learning_rate": 0.00013599746273390423,
      "loss": 1.5949,
      "step": 1010
    },
    {
      "epoch": 0.3235783963835356,
      "grad_norm": 0.7599615454673767,
      "learning_rate": 0.00013536314620995877,
      "loss": 1.3802,
      "step": 1020
    },
    {
      "epoch": 0.326750733602982,
      "grad_norm": 0.8881403207778931,
      "learning_rate": 0.00013472882968601334,
      "loss": 1.4863,
      "step": 1030
    },
    {
      "epoch": 0.32992307082242844,
      "grad_norm": 0.5106945633888245,
      "learning_rate": 0.00013409451316206787,
      "loss": 1.3598,
      "step": 1040
    },
    {
      "epoch": 0.3330954080418749,
      "grad_norm": 0.5246085524559021,
      "learning_rate": 0.00013346019663812244,
      "loss": 1.5364,
      "step": 1050
    },
    {
      "epoch": 0.3362677452613213,
      "grad_norm": 0.5905556678771973,
      "learning_rate": 0.00013282588011417698,
      "loss": 1.292,
      "step": 1060
    },
    {
      "epoch": 0.3394400824807677,
      "grad_norm": 0.6233165860176086,
      "learning_rate": 0.0001321915635902315,
      "loss": 1.6757,
      "step": 1070
    },
    {
      "epoch": 0.3426124197002141,
      "grad_norm": 0.6175003051757812,
      "learning_rate": 0.0001315572470662861,
      "loss": 1.4561,
      "step": 1080
    },
    {
      "epoch": 0.34578475691966054,
      "grad_norm": 0.7813776135444641,
      "learning_rate": 0.00013092293054234064,
      "loss": 1.4581,
      "step": 1090
    },
    {
      "epoch": 0.34895709413910697,
      "grad_norm": 0.7590473890304565,
      "learning_rate": 0.00013028861401839518,
      "loss": 1.5497,
      "step": 1100
    },
    {
      "epoch": 0.3521294313585534,
      "grad_norm": 0.44623517990112305,
      "learning_rate": 0.00012965429749444972,
      "loss": 1.4199,
      "step": 1110
    },
    {
      "epoch": 0.35530176857799983,
      "grad_norm": 0.5624171495437622,
      "learning_rate": 0.00012901998097050428,
      "loss": 1.6195,
      "step": 1120
    },
    {
      "epoch": 0.35847410579744626,
      "grad_norm": 0.7248783707618713,
      "learning_rate": 0.00012838566444655885,
      "loss": 1.4649,
      "step": 1130
    },
    {
      "epoch": 0.3616464430168927,
      "grad_norm": 0.5022657513618469,
      "learning_rate": 0.00012775134792261339,
      "loss": 1.5123,
      "step": 1140
    },
    {
      "epoch": 0.3648187802363391,
      "grad_norm": 0.5362599492073059,
      "learning_rate": 0.00012711703139866795,
      "loss": 1.5942,
      "step": 1150
    },
    {
      "epoch": 0.36799111745578555,
      "grad_norm": 0.49388712644577026,
      "learning_rate": 0.0001264827148747225,
      "loss": 1.3816,
      "step": 1160
    },
    {
      "epoch": 0.371163454675232,
      "grad_norm": 0.7559607625007629,
      "learning_rate": 0.00012584839835077705,
      "loss": 1.4344,
      "step": 1170
    },
    {
      "epoch": 0.3743357918946784,
      "grad_norm": 0.6766560673713684,
      "learning_rate": 0.0001252140818268316,
      "loss": 1.5415,
      "step": 1180
    },
    {
      "epoch": 0.37750812911412485,
      "grad_norm": 0.4876440465450287,
      "learning_rate": 0.00012457976530288615,
      "loss": 1.5882,
      "step": 1190
    },
    {
      "epoch": 0.3806804663335713,
      "grad_norm": 0.6945485472679138,
      "learning_rate": 0.0001239454487789407,
      "loss": 1.3885,
      "step": 1200
    },
    {
      "epoch": 0.3838528035530177,
      "grad_norm": 0.8747873902320862,
      "learning_rate": 0.00012331113225499523,
      "loss": 1.5543,
      "step": 1210
    },
    {
      "epoch": 0.38702514077246414,
      "grad_norm": 0.4658389389514923,
      "learning_rate": 0.00012267681573104982,
      "loss": 1.6475,
      "step": 1220
    },
    {
      "epoch": 0.3901974779919105,
      "grad_norm": 0.7519341707229614,
      "learning_rate": 0.00012204249920710436,
      "loss": 1.4127,
      "step": 1230
    },
    {
      "epoch": 0.39336981521135694,
      "grad_norm": 1.005571961402893,
      "learning_rate": 0.0001214081826831589,
      "loss": 1.4556,
      "step": 1240
    },
    {
      "epoch": 0.3965421524308034,
      "grad_norm": 0.6278214454650879,
      "learning_rate": 0.00012077386615921345,
      "loss": 1.527,
      "step": 1250
    },
    {
      "epoch": 0.3997144896502498,
      "grad_norm": 0.6354730725288391,
      "learning_rate": 0.00012013954963526801,
      "loss": 1.4456,
      "step": 1260
    },
    {
      "epoch": 0.40288682686969624,
      "grad_norm": 0.5835000872612,
      "learning_rate": 0.00011950523311132255,
      "loss": 1.4925,
      "step": 1270
    },
    {
      "epoch": 0.40605916408914267,
      "grad_norm": 0.5738991498947144,
      "learning_rate": 0.0001188709165873771,
      "loss": 1.5232,
      "step": 1280
    },
    {
      "epoch": 0.4092315013085891,
      "grad_norm": 0.5076280832290649,
      "learning_rate": 0.00011823660006343167,
      "loss": 1.761,
      "step": 1290
    },
    {
      "epoch": 0.4124038385280355,
      "grad_norm": 0.570414125919342,
      "learning_rate": 0.00011760228353948622,
      "loss": 1.4488,
      "step": 1300
    },
    {
      "epoch": 0.41557617574748196,
      "grad_norm": 0.6557534337043762,
      "learning_rate": 0.00011696796701554076,
      "loss": 1.4647,
      "step": 1310
    },
    {
      "epoch": 0.4187485129669284,
      "grad_norm": 0.5542351603507996,
      "learning_rate": 0.00011633365049159531,
      "loss": 1.5478,
      "step": 1320
    },
    {
      "epoch": 0.4219208501863748,
      "grad_norm": 0.6032525300979614,
      "learning_rate": 0.00011569933396764987,
      "loss": 1.4288,
      "step": 1330
    },
    {
      "epoch": 0.42509318740582125,
      "grad_norm": 1.1610643863677979,
      "learning_rate": 0.00011506501744370441,
      "loss": 1.3006,
      "step": 1340
    },
    {
      "epoch": 0.4282655246252677,
      "grad_norm": 0.5841925144195557,
      "learning_rate": 0.00011443070091975896,
      "loss": 1.3774,
      "step": 1350
    },
    {
      "epoch": 0.4314378618447141,
      "grad_norm": 0.7486493587493896,
      "learning_rate": 0.00011379638439581353,
      "loss": 1.3341,
      "step": 1360
    },
    {
      "epoch": 0.43461019906416054,
      "grad_norm": 0.5252776145935059,
      "learning_rate": 0.00011316206787186808,
      "loss": 1.3439,
      "step": 1370
    },
    {
      "epoch": 0.43778253628360697,
      "grad_norm": 0.6856101751327515,
      "learning_rate": 0.00011252775134792261,
      "loss": 1.4777,
      "step": 1380
    },
    {
      "epoch": 0.4409548735030534,
      "grad_norm": 0.8473894000053406,
      "learning_rate": 0.00011189343482397717,
      "loss": 1.5198,
      "step": 1390
    },
    {
      "epoch": 0.4441272107224998,
      "grad_norm": 0.556738555431366,
      "learning_rate": 0.00011125911830003173,
      "loss": 1.3857,
      "step": 1400
    },
    {
      "epoch": 0.4472995479419462,
      "grad_norm": 0.651705801486969,
      "learning_rate": 0.00011062480177608627,
      "loss": 1.4674,
      "step": 1410
    },
    {
      "epoch": 0.45047188516139264,
      "grad_norm": 1.0560656785964966,
      "learning_rate": 0.00010999048525214082,
      "loss": 1.4841,
      "step": 1420
    },
    {
      "epoch": 0.45364422238083907,
      "grad_norm": 0.5865368843078613,
      "learning_rate": 0.00010935616872819538,
      "loss": 1.3677,
      "step": 1430
    },
    {
      "epoch": 0.4568165596002855,
      "grad_norm": 0.5501188039779663,
      "learning_rate": 0.00010872185220424994,
      "loss": 1.5191,
      "step": 1440
    },
    {
      "epoch": 0.45998889681973193,
      "grad_norm": 0.7201678156852722,
      "learning_rate": 0.00010808753568030447,
      "loss": 1.457,
      "step": 1450
    },
    {
      "epoch": 0.46316123403917836,
      "grad_norm": 0.6705619096755981,
      "learning_rate": 0.00010745321915635902,
      "loss": 1.3525,
      "step": 1460
    },
    {
      "epoch": 0.4663335712586248,
      "grad_norm": 0.6667531728744507,
      "learning_rate": 0.00010681890263241359,
      "loss": 1.4112,
      "step": 1470
    },
    {
      "epoch": 0.4695059084780712,
      "grad_norm": 0.6039954423904419,
      "learning_rate": 0.00010618458610846813,
      "loss": 1.5011,
      "step": 1480
    },
    {
      "epoch": 0.47267824569751765,
      "grad_norm": 0.890130877494812,
      "learning_rate": 0.00010555026958452268,
      "loss": 1.3452,
      "step": 1490
    },
    {
      "epoch": 0.4758505829169641,
      "grad_norm": 1.0962255001068115,
      "learning_rate": 0.00010491595306057724,
      "loss": 1.382,
      "step": 1500
    },
    {
      "epoch": 0.4790229201364105,
      "grad_norm": 0.7490684986114502,
      "learning_rate": 0.0001042816365366318,
      "loss": 1.4722,
      "step": 1510
    },
    {
      "epoch": 0.48219525735585694,
      "grad_norm": 0.8900219202041626,
      "learning_rate": 0.00010364732001268633,
      "loss": 1.3786,
      "step": 1520
    },
    {
      "epoch": 0.4853675945753034,
      "grad_norm": 0.7582489252090454,
      "learning_rate": 0.00010301300348874088,
      "loss": 1.3516,
      "step": 1530
    },
    {
      "epoch": 0.4885399317947498,
      "grad_norm": 0.5815539360046387,
      "learning_rate": 0.00010237868696479545,
      "loss": 1.613,
      "step": 1540
    },
    {
      "epoch": 0.49171226901419623,
      "grad_norm": 0.7492201924324036,
      "learning_rate": 0.00010174437044084999,
      "loss": 1.3882,
      "step": 1550
    },
    {
      "epoch": 0.4948846062336426,
      "grad_norm": 1.0547064542770386,
      "learning_rate": 0.00010111005391690454,
      "loss": 1.5411,
      "step": 1560
    },
    {
      "epoch": 0.49805694345308904,
      "grad_norm": 0.6500411629676819,
      "learning_rate": 0.0001004757373929591,
      "loss": 1.3937,
      "step": 1570
    },
    {
      "epoch": 0.5012292806725355,
      "grad_norm": 0.6362413167953491,
      "learning_rate": 9.984142086901365e-05,
      "loss": 1.6401,
      "step": 1580
    },
    {
      "epoch": 0.5044016178919819,
      "grad_norm": 0.5573274493217468,
      "learning_rate": 9.920710434506819e-05,
      "loss": 1.4127,
      "step": 1590
    },
    {
      "epoch": 0.5075739551114283,
      "grad_norm": 1.144141674041748,
      "learning_rate": 9.857278782112274e-05,
      "loss": 1.3695,
      "step": 1600
    },
    {
      "epoch": 0.5107462923308748,
      "grad_norm": 0.5378378033638,
      "learning_rate": 9.793847129717729e-05,
      "loss": 1.4593,
      "step": 1610
    },
    {
      "epoch": 0.5139186295503212,
      "grad_norm": 0.7252602577209473,
      "learning_rate": 9.730415477323184e-05,
      "loss": 1.3318,
      "step": 1620
    },
    {
      "epoch": 0.5170909667697676,
      "grad_norm": 0.7939114570617676,
      "learning_rate": 9.666983824928641e-05,
      "loss": 1.64,
      "step": 1630
    },
    {
      "epoch": 0.520263303989214,
      "grad_norm": 0.6520329117774963,
      "learning_rate": 9.603552172534095e-05,
      "loss": 1.4673,
      "step": 1640
    },
    {
      "epoch": 0.5234356412086605,
      "grad_norm": 0.5876366496086121,
      "learning_rate": 9.540120520139551e-05,
      "loss": 1.4294,
      "step": 1650
    },
    {
      "epoch": 0.5266079784281069,
      "grad_norm": 0.5981981158256531,
      "learning_rate": 9.476688867745005e-05,
      "loss": 1.4096,
      "step": 1660
    },
    {
      "epoch": 0.5297803156475533,
      "grad_norm": 0.658304750919342,
      "learning_rate": 9.41325721535046e-05,
      "loss": 1.478,
      "step": 1670
    },
    {
      "epoch": 0.5329526528669998,
      "grad_norm": 0.6807225942611694,
      "learning_rate": 9.349825562955915e-05,
      "loss": 1.2804,
      "step": 1680
    },
    {
      "epoch": 0.5361249900864462,
      "grad_norm": 0.58089280128479,
      "learning_rate": 9.28639391056137e-05,
      "loss": 1.4702,
      "step": 1690
    },
    {
      "epoch": 0.5392973273058926,
      "grad_norm": 0.707040548324585,
      "learning_rate": 9.222962258166827e-05,
      "loss": 1.3937,
      "step": 1700
    },
    {
      "epoch": 0.5424696645253391,
      "grad_norm": 0.5643823742866516,
      "learning_rate": 9.15953060577228e-05,
      "loss": 1.3507,
      "step": 1710
    },
    {
      "epoch": 0.5456420017447855,
      "grad_norm": 0.6909765601158142,
      "learning_rate": 9.096098953377737e-05,
      "loss": 1.523,
      "step": 1720
    },
    {
      "epoch": 0.5488143389642319,
      "grad_norm": 0.512374758720398,
      "learning_rate": 9.032667300983191e-05,
      "loss": 1.4415,
      "step": 1730
    },
    {
      "epoch": 0.5519866761836784,
      "grad_norm": 0.6620744466781616,
      "learning_rate": 8.969235648588646e-05,
      "loss": 1.6159,
      "step": 1740
    },
    {
      "epoch": 0.5551590134031248,
      "grad_norm": 0.7365561127662659,
      "learning_rate": 8.905803996194101e-05,
      "loss": 1.443,
      "step": 1750
    },
    {
      "epoch": 0.5583313506225712,
      "grad_norm": 0.7002434134483337,
      "learning_rate": 8.842372343799556e-05,
      "loss": 1.4088,
      "step": 1760
    },
    {
      "epoch": 0.5615036878420177,
      "grad_norm": 0.6654956936836243,
      "learning_rate": 8.778940691405013e-05,
      "loss": 1.547,
      "step": 1770
    },
    {
      "epoch": 0.5646760250614641,
      "grad_norm": 0.9390401840209961,
      "learning_rate": 8.715509039010466e-05,
      "loss": 1.541,
      "step": 1780
    },
    {
      "epoch": 0.5678483622809105,
      "grad_norm": 0.546926736831665,
      "learning_rate": 8.652077386615922e-05,
      "loss": 1.3852,
      "step": 1790
    },
    {
      "epoch": 0.5710206995003569,
      "grad_norm": 0.5738383531570435,
      "learning_rate": 8.588645734221377e-05,
      "loss": 1.4224,
      "step": 1800
    },
    {
      "epoch": 0.5741930367198033,
      "grad_norm": 0.5850147604942322,
      "learning_rate": 8.525214081826832e-05,
      "loss": 1.3388,
      "step": 1810
    },
    {
      "epoch": 0.5773653739392497,
      "grad_norm": 0.4137985408306122,
      "learning_rate": 8.461782429432287e-05,
      "loss": 1.464,
      "step": 1820
    },
    {
      "epoch": 0.5805377111586961,
      "grad_norm": 0.5856068730354309,
      "learning_rate": 8.398350777037742e-05,
      "loss": 1.488,
      "step": 1830
    },
    {
      "epoch": 0.5837100483781426,
      "grad_norm": 0.7041917443275452,
      "learning_rate": 8.334919124643198e-05,
      "loss": 1.3592,
      "step": 1840
    },
    {
      "epoch": 0.586882385597589,
      "grad_norm": 0.5915296673774719,
      "learning_rate": 8.271487472248652e-05,
      "loss": 1.3984,
      "step": 1850
    },
    {
      "epoch": 0.5900547228170354,
      "grad_norm": 0.4675104022026062,
      "learning_rate": 8.208055819854107e-05,
      "loss": 1.2745,
      "step": 1860
    },
    {
      "epoch": 0.5932270600364818,
      "grad_norm": 0.8226433396339417,
      "learning_rate": 8.144624167459562e-05,
      "loss": 1.5216,
      "step": 1870
    },
    {
      "epoch": 0.5963993972559283,
      "grad_norm": 0.7295730113983154,
      "learning_rate": 8.081192515065018e-05,
      "loss": 1.4765,
      "step": 1880
    },
    {
      "epoch": 0.5995717344753747,
      "grad_norm": 0.765569806098938,
      "learning_rate": 8.017760862670473e-05,
      "loss": 1.2865,
      "step": 1890
    },
    {
      "epoch": 0.6027440716948211,
      "grad_norm": 0.6398757696151733,
      "learning_rate": 7.954329210275928e-05,
      "loss": 1.4888,
      "step": 1900
    },
    {
      "epoch": 0.6059164089142676,
      "grad_norm": 0.7606919407844543,
      "learning_rate": 7.890897557881384e-05,
      "loss": 1.2592,
      "step": 1910
    },
    {
      "epoch": 0.609088746133714,
      "grad_norm": 0.7983245253562927,
      "learning_rate": 7.827465905486838e-05,
      "loss": 1.5687,
      "step": 1920
    },
    {
      "epoch": 0.6122610833531604,
      "grad_norm": 1.0482501983642578,
      "learning_rate": 7.764034253092293e-05,
      "loss": 1.5696,
      "step": 1930
    },
    {
      "epoch": 0.6154334205726069,
      "grad_norm": 0.7727607488632202,
      "learning_rate": 7.700602600697748e-05,
      "loss": 1.3695,
      "step": 1940
    },
    {
      "epoch": 0.6186057577920533,
      "grad_norm": 0.5950964093208313,
      "learning_rate": 7.637170948303203e-05,
      "loss": 1.4736,
      "step": 1950
    },
    {
      "epoch": 0.6217780950114997,
      "grad_norm": 0.5348950028419495,
      "learning_rate": 7.573739295908659e-05,
      "loss": 1.3901,
      "step": 1960
    },
    {
      "epoch": 0.6249504322309462,
      "grad_norm": 0.7363529801368713,
      "learning_rate": 7.510307643514114e-05,
      "loss": 1.3903,
      "step": 1970
    },
    {
      "epoch": 0.6281227694503926,
      "grad_norm": 0.7907599806785583,
      "learning_rate": 7.44687599111957e-05,
      "loss": 1.5368,
      "step": 1980
    },
    {
      "epoch": 0.631295106669839,
      "grad_norm": 0.7850868105888367,
      "learning_rate": 7.383444338725024e-05,
      "loss": 1.5949,
      "step": 1990
    },
    {
      "epoch": 0.6344674438892854,
      "grad_norm": 0.6751273274421692,
      "learning_rate": 7.320012686330479e-05,
      "loss": 1.4821,
      "step": 2000
    },
    {
      "epoch": 0.6376397811087319,
      "grad_norm": 0.5463380217552185,
      "learning_rate": 7.256581033935934e-05,
      "loss": 1.3566,
      "step": 2010
    },
    {
      "epoch": 0.6408121183281783,
      "grad_norm": 0.4443261921405792,
      "learning_rate": 7.19314938154139e-05,
      "loss": 1.3974,
      "step": 2020
    },
    {
      "epoch": 0.6439844555476247,
      "grad_norm": 0.5925237536430359,
      "learning_rate": 7.129717729146844e-05,
      "loss": 1.4647,
      "step": 2030
    },
    {
      "epoch": 0.6471567927670712,
      "grad_norm": 0.6692027449607849,
      "learning_rate": 7.0662860767523e-05,
      "loss": 1.3983,
      "step": 2040
    },
    {
      "epoch": 0.6503291299865176,
      "grad_norm": 0.6457743048667908,
      "learning_rate": 7.002854424357756e-05,
      "loss": 1.4938,
      "step": 2050
    },
    {
      "epoch": 0.653501467205964,
      "grad_norm": 0.8625224828720093,
      "learning_rate": 6.93942277196321e-05,
      "loss": 1.4055,
      "step": 2060
    },
    {
      "epoch": 0.6566738044254105,
      "grad_norm": 0.5152676105499268,
      "learning_rate": 6.875991119568665e-05,
      "loss": 1.2916,
      "step": 2070
    },
    {
      "epoch": 0.6598461416448569,
      "grad_norm": 0.7213692665100098,
      "learning_rate": 6.81255946717412e-05,
      "loss": 1.3232,
      "step": 2080
    },
    {
      "epoch": 0.6630184788643033,
      "grad_norm": 0.9094134569168091,
      "learning_rate": 6.749127814779575e-05,
      "loss": 1.4663,
      "step": 2090
    },
    {
      "epoch": 0.6661908160837497,
      "grad_norm": 0.8411860466003418,
      "learning_rate": 6.68569616238503e-05,
      "loss": 1.2783,
      "step": 2100
    },
    {
      "epoch": 0.6693631533031962,
      "grad_norm": 0.728233277797699,
      "learning_rate": 6.622264509990485e-05,
      "loss": 1.4997,
      "step": 2110
    },
    {
      "epoch": 0.6725354905226426,
      "grad_norm": 0.6808672547340393,
      "learning_rate": 6.558832857595942e-05,
      "loss": 1.63,
      "step": 2120
    },
    {
      "epoch": 0.6757078277420889,
      "grad_norm": 0.7046238780021667,
      "learning_rate": 6.495401205201396e-05,
      "loss": 1.3517,
      "step": 2130
    },
    {
      "epoch": 0.6788801649615354,
      "grad_norm": 0.7599073052406311,
      "learning_rate": 6.431969552806851e-05,
      "loss": 1.4406,
      "step": 2140
    },
    {
      "epoch": 0.6820525021809818,
      "grad_norm": 0.9384264349937439,
      "learning_rate": 6.368537900412306e-05,
      "loss": 1.4006,
      "step": 2150
    },
    {
      "epoch": 0.6852248394004282,
      "grad_norm": 0.5403696298599243,
      "learning_rate": 6.305106248017761e-05,
      "loss": 1.3984,
      "step": 2160
    },
    {
      "epoch": 0.6883971766198747,
      "grad_norm": 0.8392882347106934,
      "learning_rate": 6.241674595623216e-05,
      "loss": 1.4558,
      "step": 2170
    },
    {
      "epoch": 0.6915695138393211,
      "grad_norm": 0.6106340885162354,
      "learning_rate": 6.178242943228671e-05,
      "loss": 1.4803,
      "step": 2180
    },
    {
      "epoch": 0.6947418510587675,
      "grad_norm": 0.6465825438499451,
      "learning_rate": 6.114811290834126e-05,
      "loss": 1.4991,
      "step": 2190
    },
    {
      "epoch": 0.6979141882782139,
      "grad_norm": 1.0067543983459473,
      "learning_rate": 6.0513796384395815e-05,
      "loss": 1.2839,
      "step": 2200
    },
    {
      "epoch": 0.7010865254976604,
      "grad_norm": 0.94963139295578,
      "learning_rate": 5.9879479860450373e-05,
      "loss": 1.5481,
      "step": 2210
    },
    {
      "epoch": 0.7042588627171068,
      "grad_norm": 0.7615886330604553,
      "learning_rate": 5.924516333650492e-05,
      "loss": 1.3871,
      "step": 2220
    },
    {
      "epoch": 0.7074311999365532,
      "grad_norm": 0.7721972465515137,
      "learning_rate": 5.861084681255947e-05,
      "loss": 1.4102,
      "step": 2230
    },
    {
      "epoch": 0.7106035371559997,
      "grad_norm": 0.7574743032455444,
      "learning_rate": 5.797653028861402e-05,
      "loss": 1.486,
      "step": 2240
    },
    {
      "epoch": 0.7137758743754461,
      "grad_norm": 0.6218211650848389,
      "learning_rate": 5.734221376466857e-05,
      "loss": 1.6237,
      "step": 2250
    },
    {
      "epoch": 0.7169482115948925,
      "grad_norm": 0.723132312297821,
      "learning_rate": 5.670789724072313e-05,
      "loss": 1.3851,
      "step": 2260
    },
    {
      "epoch": 0.720120548814339,
      "grad_norm": 0.7903152704238892,
      "learning_rate": 5.6073580716777674e-05,
      "loss": 1.4266,
      "step": 2270
    },
    {
      "epoch": 0.7232928860337854,
      "grad_norm": 0.7292097806930542,
      "learning_rate": 5.543926419283223e-05,
      "loss": 1.4201,
      "step": 2280
    },
    {
      "epoch": 0.7264652232532318,
      "grad_norm": 1.3899266719818115,
      "learning_rate": 5.4804947668886776e-05,
      "loss": 1.3649,
      "step": 2290
    },
    {
      "epoch": 0.7296375604726782,
      "grad_norm": 0.7194644212722778,
      "learning_rate": 5.417063114494133e-05,
      "loss": 1.3478,
      "step": 2300
    },
    {
      "epoch": 0.7328098976921247,
      "grad_norm": 0.7551056742668152,
      "learning_rate": 5.353631462099587e-05,
      "loss": 1.2969,
      "step": 2310
    },
    {
      "epoch": 0.7359822349115711,
      "grad_norm": 0.800243079662323,
      "learning_rate": 5.290199809705043e-05,
      "loss": 1.3995,
      "step": 2320
    },
    {
      "epoch": 0.7391545721310175,
      "grad_norm": 0.6010524034500122,
      "learning_rate": 5.226768157310499e-05,
      "loss": 1.3131,
      "step": 2330
    },
    {
      "epoch": 0.742326909350464,
      "grad_norm": 0.9440648555755615,
      "learning_rate": 5.163336504915953e-05,
      "loss": 1.4598,
      "step": 2340
    },
    {
      "epoch": 0.7454992465699104,
      "grad_norm": 0.6158448457717896,
      "learning_rate": 5.099904852521409e-05,
      "loss": 1.5768,
      "step": 2350
    },
    {
      "epoch": 0.7486715837893568,
      "grad_norm": 0.6335797309875488,
      "learning_rate": 5.0364732001268635e-05,
      "loss": 1.5131,
      "step": 2360
    },
    {
      "epoch": 0.7518439210088033,
      "grad_norm": 0.5561012625694275,
      "learning_rate": 4.9730415477323186e-05,
      "loss": 1.2195,
      "step": 2370
    },
    {
      "epoch": 0.7550162582282497,
      "grad_norm": 0.5602462291717529,
      "learning_rate": 4.909609895337774e-05,
      "loss": 1.5301,
      "step": 2380
    },
    {
      "epoch": 0.7581885954476961,
      "grad_norm": 0.8307023644447327,
      "learning_rate": 4.846178242943229e-05,
      "loss": 1.5202,
      "step": 2390
    },
    {
      "epoch": 0.7613609326671426,
      "grad_norm": 0.7464556097984314,
      "learning_rate": 4.782746590548684e-05,
      "loss": 1.3684,
      "step": 2400
    },
    {
      "epoch": 0.764533269886589,
      "grad_norm": 0.6019208431243896,
      "learning_rate": 4.719314938154139e-05,
      "loss": 1.6312,
      "step": 2410
    },
    {
      "epoch": 0.7677056071060354,
      "grad_norm": 0.6791258454322815,
      "learning_rate": 4.655883285759594e-05,
      "loss": 1.3608,
      "step": 2420
    },
    {
      "epoch": 0.7708779443254818,
      "grad_norm": 1.0024809837341309,
      "learning_rate": 4.5924516333650494e-05,
      "loss": 1.4769,
      "step": 2430
    },
    {
      "epoch": 0.7740502815449283,
      "grad_norm": 0.5239784717559814,
      "learning_rate": 4.5290199809705045e-05,
      "loss": 1.4124,
      "step": 2440
    },
    {
      "epoch": 0.7772226187643747,
      "grad_norm": 0.7941654920578003,
      "learning_rate": 4.4655883285759596e-05,
      "loss": 1.5665,
      "step": 2450
    },
    {
      "epoch": 0.780394955983821,
      "grad_norm": 0.737298309803009,
      "learning_rate": 4.402156676181415e-05,
      "loss": 1.5099,
      "step": 2460
    },
    {
      "epoch": 0.7835672932032675,
      "grad_norm": 0.49148768186569214,
      "learning_rate": 4.33872502378687e-05,
      "loss": 1.5797,
      "step": 2470
    },
    {
      "epoch": 0.7867396304227139,
      "grad_norm": 0.7500898241996765,
      "learning_rate": 4.275293371392325e-05,
      "loss": 1.5557,
      "step": 2480
    },
    {
      "epoch": 0.7899119676421603,
      "grad_norm": 0.6595982909202576,
      "learning_rate": 4.21186171899778e-05,
      "loss": 1.423,
      "step": 2490
    },
    {
      "epoch": 0.7930843048616067,
      "grad_norm": 1.045883059501648,
      "learning_rate": 4.148430066603235e-05,
      "loss": 1.3953,
      "step": 2500
    },
    {
      "epoch": 0.7962566420810532,
      "grad_norm": 0.6625130772590637,
      "learning_rate": 4.0849984142086904e-05,
      "loss": 1.4428,
      "step": 2510
    },
    {
      "epoch": 0.7994289793004996,
      "grad_norm": 0.8583341836929321,
      "learning_rate": 4.0215667618141455e-05,
      "loss": 1.458,
      "step": 2520
    },
    {
      "epoch": 0.802601316519946,
      "grad_norm": 0.6073096394538879,
      "learning_rate": 3.9581351094196006e-05,
      "loss": 1.4958,
      "step": 2530
    },
    {
      "epoch": 0.8057736537393925,
      "grad_norm": 0.6754409670829773,
      "learning_rate": 3.894703457025056e-05,
      "loss": 1.4683,
      "step": 2540
    },
    {
      "epoch": 0.8089459909588389,
      "grad_norm": 1.0219086408615112,
      "learning_rate": 3.831271804630511e-05,
      "loss": 1.6865,
      "step": 2550
    },
    {
      "epoch": 0.8121183281782853,
      "grad_norm": 0.7296629548072815,
      "learning_rate": 3.767840152235966e-05,
      "loss": 1.4101,
      "step": 2560
    },
    {
      "epoch": 0.8152906653977318,
      "grad_norm": 0.7084376811981201,
      "learning_rate": 3.704408499841421e-05,
      "loss": 1.3687,
      "step": 2570
    },
    {
      "epoch": 0.8184630026171782,
      "grad_norm": 0.6839606761932373,
      "learning_rate": 3.640976847446876e-05,
      "loss": 1.2874,
      "step": 2580
    },
    {
      "epoch": 0.8216353398366246,
      "grad_norm": 0.5827651023864746,
      "learning_rate": 3.577545195052331e-05,
      "loss": 1.4287,
      "step": 2590
    },
    {
      "epoch": 0.824807677056071,
      "grad_norm": 0.8228484988212585,
      "learning_rate": 3.5141135426577865e-05,
      "loss": 1.5443,
      "step": 2600
    },
    {
      "epoch": 0.8279800142755175,
      "grad_norm": 0.6675695776939392,
      "learning_rate": 3.4506818902632416e-05,
      "loss": 1.3595,
      "step": 2610
    },
    {
      "epoch": 0.8311523514949639,
      "grad_norm": 0.6560317873954773,
      "learning_rate": 3.387250237868697e-05,
      "loss": 1.6734,
      "step": 2620
    },
    {
      "epoch": 0.8343246887144103,
      "grad_norm": 0.7621636986732483,
      "learning_rate": 3.323818585474152e-05,
      "loss": 1.5401,
      "step": 2630
    },
    {
      "epoch": 0.8374970259338568,
      "grad_norm": 1.0541629791259766,
      "learning_rate": 3.260386933079607e-05,
      "loss": 1.3251,
      "step": 2640
    },
    {
      "epoch": 0.8406693631533032,
      "grad_norm": 0.8039554357528687,
      "learning_rate": 3.196955280685062e-05,
      "loss": 1.5132,
      "step": 2650
    },
    {
      "epoch": 0.8438417003727496,
      "grad_norm": 0.6521329879760742,
      "learning_rate": 3.133523628290517e-05,
      "loss": 1.3429,
      "step": 2660
    },
    {
      "epoch": 0.8470140375921961,
      "grad_norm": 0.721842885017395,
      "learning_rate": 3.070091975895972e-05,
      "loss": 1.4139,
      "step": 2670
    },
    {
      "epoch": 0.8501863748116425,
      "grad_norm": 0.916768491268158,
      "learning_rate": 3.0066603235014274e-05,
      "loss": 1.4847,
      "step": 2680
    },
    {
      "epoch": 0.8533587120310889,
      "grad_norm": 0.8975218534469604,
      "learning_rate": 2.9432286711068822e-05,
      "loss": 1.4601,
      "step": 2690
    },
    {
      "epoch": 0.8565310492505354,
      "grad_norm": 0.597590982913971,
      "learning_rate": 2.8797970187123374e-05,
      "loss": 1.2949,
      "step": 2700
    },
    {
      "epoch": 0.8597033864699818,
      "grad_norm": 0.7980623841285706,
      "learning_rate": 2.8163653663177925e-05,
      "loss": 1.2911,
      "step": 2710
    },
    {
      "epoch": 0.8628757236894282,
      "grad_norm": 0.7135227918624878,
      "learning_rate": 2.752933713923248e-05,
      "loss": 1.4295,
      "step": 2720
    },
    {
      "epoch": 0.8660480609088747,
      "grad_norm": 0.7329544425010681,
      "learning_rate": 2.689502061528703e-05,
      "loss": 1.5118,
      "step": 2730
    },
    {
      "epoch": 0.8692203981283211,
      "grad_norm": 0.6412659883499146,
      "learning_rate": 2.6260704091341582e-05,
      "loss": 1.6107,
      "step": 2740
    },
    {
      "epoch": 0.8723927353477675,
      "grad_norm": 0.4782559275627136,
      "learning_rate": 2.5626387567396133e-05,
      "loss": 1.3337,
      "step": 2750
    },
    {
      "epoch": 0.8755650725672139,
      "grad_norm": 0.5139066576957703,
      "learning_rate": 2.499207104345068e-05,
      "loss": 1.3932,
      "step": 2760
    },
    {
      "epoch": 0.8787374097866604,
      "grad_norm": 0.7327658534049988,
      "learning_rate": 2.4357754519505236e-05,
      "loss": 1.2583,
      "step": 2770
    },
    {
      "epoch": 0.8819097470061068,
      "grad_norm": 0.8453318476676941,
      "learning_rate": 2.3723437995559787e-05,
      "loss": 1.3443,
      "step": 2780
    },
    {
      "epoch": 0.8850820842255531,
      "grad_norm": 0.6334735155105591,
      "learning_rate": 2.3089121471614335e-05,
      "loss": 1.3454,
      "step": 2790
    },
    {
      "epoch": 0.8882544214449996,
      "grad_norm": 0.5725166201591492,
      "learning_rate": 2.2454804947668886e-05,
      "loss": 1.4639,
      "step": 2800
    },
    {
      "epoch": 0.891426758664446,
      "grad_norm": 0.6995698809623718,
      "learning_rate": 2.182048842372344e-05,
      "loss": 1.2928,
      "step": 2810
    },
    {
      "epoch": 0.8945990958838924,
      "grad_norm": 0.6511154174804688,
      "learning_rate": 2.118617189977799e-05,
      "loss": 1.3316,
      "step": 2820
    },
    {
      "epoch": 0.8977714331033388,
      "grad_norm": 0.7156962156295776,
      "learning_rate": 2.055185537583254e-05,
      "loss": 1.411,
      "step": 2830
    },
    {
      "epoch": 0.9009437703227853,
      "grad_norm": 0.6765316724777222,
      "learning_rate": 1.9917538851887094e-05,
      "loss": 1.4247,
      "step": 2840
    },
    {
      "epoch": 0.9041161075422317,
      "grad_norm": 0.7900785803794861,
      "learning_rate": 1.9283222327941645e-05,
      "loss": 1.5267,
      "step": 2850
    },
    {
      "epoch": 0.9072884447616781,
      "grad_norm": 0.6453720927238464,
      "learning_rate": 1.8648905803996193e-05,
      "loss": 1.3115,
      "step": 2860
    },
    {
      "epoch": 0.9104607819811246,
      "grad_norm": 0.8476659059524536,
      "learning_rate": 1.8014589280050744e-05,
      "loss": 1.2354,
      "step": 2870
    },
    {
      "epoch": 0.913633119200571,
      "grad_norm": 0.5731165409088135,
      "learning_rate": 1.73802727561053e-05,
      "loss": 1.4819,
      "step": 2880
    },
    {
      "epoch": 0.9168054564200174,
      "grad_norm": 0.8955910801887512,
      "learning_rate": 1.6745956232159847e-05,
      "loss": 1.3838,
      "step": 2890
    },
    {
      "epoch": 0.9199777936394639,
      "grad_norm": 0.7124281525611877,
      "learning_rate": 1.6111639708214398e-05,
      "loss": 1.3306,
      "step": 2900
    },
    {
      "epoch": 0.9231501308589103,
      "grad_norm": 0.6509839296340942,
      "learning_rate": 1.5477323184268953e-05,
      "loss": 1.5545,
      "step": 2910
    },
    {
      "epoch": 0.9263224680783567,
      "grad_norm": 0.5433329939842224,
      "learning_rate": 1.4843006660323502e-05,
      "loss": 1.4086,
      "step": 2920
    },
    {
      "epoch": 0.9294948052978032,
      "grad_norm": 0.9790749549865723,
      "learning_rate": 1.4208690136378053e-05,
      "loss": 1.4428,
      "step": 2930
    },
    {
      "epoch": 0.9326671425172496,
      "grad_norm": 0.8580828309059143,
      "learning_rate": 1.3574373612432603e-05,
      "loss": 1.3097,
      "step": 2940
    },
    {
      "epoch": 0.935839479736696,
      "grad_norm": 0.8969557285308838,
      "learning_rate": 1.2940057088487156e-05,
      "loss": 1.4705,
      "step": 2950
    },
    {
      "epoch": 0.9390118169561424,
      "grad_norm": 0.6240678429603577,
      "learning_rate": 1.2305740564541707e-05,
      "loss": 1.4382,
      "step": 2960
    },
    {
      "epoch": 0.9421841541755889,
      "grad_norm": 0.7679318189620972,
      "learning_rate": 1.1671424040596258e-05,
      "loss": 1.481,
      "step": 2970
    },
    {
      "epoch": 0.9453564913950353,
      "grad_norm": 0.6141589879989624,
      "learning_rate": 1.103710751665081e-05,
      "loss": 1.6051,
      "step": 2980
    },
    {
      "epoch": 0.9485288286144817,
      "grad_norm": 0.6854019165039062,
      "learning_rate": 1.0402790992705361e-05,
      "loss": 1.2704,
      "step": 2990
    },
    {
      "epoch": 0.9517011658339282,
      "grad_norm": 0.7521488666534424,
      "learning_rate": 9.76847446875991e-06,
      "loss": 1.3217,
      "step": 3000
    },
    {
      "epoch": 0.9548735030533746,
      "grad_norm": 0.835254967212677,
      "learning_rate": 9.134157944814463e-06,
      "loss": 1.4214,
      "step": 3010
    },
    {
      "epoch": 0.958045840272821,
      "grad_norm": 0.6488634347915649,
      "learning_rate": 8.499841420869015e-06,
      "loss": 1.5155,
      "step": 3020
    },
    {
      "epoch": 0.9612181774922675,
      "grad_norm": 0.8736590147018433,
      "learning_rate": 7.865524896923566e-06,
      "loss": 1.5992,
      "step": 3030
    },
    {
      "epoch": 0.9643905147117139,
      "grad_norm": 0.6156706809997559,
      "learning_rate": 7.231208372978117e-06,
      "loss": 1.4738,
      "step": 3040
    },
    {
      "epoch": 0.9675628519311603,
      "grad_norm": 0.6207919120788574,
      "learning_rate": 6.596891849032667e-06,
      "loss": 1.3097,
      "step": 3050
    },
    {
      "epoch": 0.9707351891506067,
      "grad_norm": 0.8472728729248047,
      "learning_rate": 5.9625753250872195e-06,
      "loss": 1.3702,
      "step": 3060
    },
    {
      "epoch": 0.9739075263700532,
      "grad_norm": 0.5944570899009705,
      "learning_rate": 5.32825880114177e-06,
      "loss": 1.3779,
      "step": 3070
    },
    {
      "epoch": 0.9770798635894996,
      "grad_norm": 0.588695764541626,
      "learning_rate": 4.693942277196321e-06,
      "loss": 1.4046,
      "step": 3080
    },
    {
      "epoch": 0.980252200808946,
      "grad_norm": 0.9806285500526428,
      "learning_rate": 4.059625753250872e-06,
      "loss": 1.4783,
      "step": 3090
    },
    {
      "epoch": 0.9834245380283925,
      "grad_norm": 0.46336981654167175,
      "learning_rate": 3.4253092293054235e-06,
      "loss": 1.5113,
      "step": 3100
    },
    {
      "epoch": 0.9865968752478389,
      "grad_norm": 0.5777629017829895,
      "learning_rate": 2.7909927053599748e-06,
      "loss": 1.4211,
      "step": 3110
    },
    {
      "epoch": 0.9897692124672852,
      "grad_norm": 0.5523709654808044,
      "learning_rate": 2.156676181414526e-06,
      "loss": 1.4778,
      "step": 3120
    },
    {
      "epoch": 0.9929415496867317,
      "grad_norm": 0.8593897819519043,
      "learning_rate": 1.5223596574690772e-06,
      "loss": 1.4368,
      "step": 3130
    },
    {
      "epoch": 0.9961138869061781,
      "grad_norm": 0.5587384104728699,
      "learning_rate": 8.880431335236283e-07,
      "loss": 1.3423,
      "step": 3140
    },
    {
      "epoch": 0.9992862241256245,
      "grad_norm": 0.6164531707763672,
      "learning_rate": 2.537266095781795e-07,
      "loss": 1.5348,
      "step": 3150
    }
  ],
  "logging_steps": 10,
  "max_steps": 3153,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0243876655097446e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
